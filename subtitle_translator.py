#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å­—å¹•æ™ºèƒ½ç¿»è¯‘å·¥å…·ä¸»ç¨‹åº

æ€§èƒ½ä¼˜åŒ–å»ºè®®:
1. å¢å¤§æ‰¹å¤„ç†å¤§å°å’Œå‡å°‘æ‰¹æ¬¡å»¶è¿Ÿå¯ä»¥æé«˜é€Ÿåº¦ï¼Œä½†éœ€æ³¨æ„APIé™åˆ¶
2. åœ¨èµ„æºå…è®¸çš„æƒ…å†µä¸‹ï¼Œå¢åŠ worker_countå¯ä»¥æ˜¾è‘—æé«˜ç¿»è¯‘é€Ÿåº¦
3. åœºæ™¯è¾¹ç•Œæ£€æµ‹å¯ä»¥ä¼˜åŒ–ä¸ºä½¿ç”¨äºŒåˆ†æ³•ï¼Œæé«˜å¤§å‹å­—å¹•æ–‡ä»¶çš„å¤„ç†é€Ÿåº¦
4. å¯¹äºéå¸¸å¤§çš„å­—å¹•æ–‡ä»¶(>3000è¡Œ)ï¼Œå¯è€ƒè™‘æ‰©å¤§chunk_sizeä»¥å‡å°‘APIè°ƒç”¨æ¬¡æ•°
5. å‘é‡ç¼“å­˜çš„ä½¿ç”¨å¯ä»¥æ˜¾è‘—æé«˜é‡å¤å¤„ç†é€Ÿåº¦ï¼Œå»ºè®®åœ¨é‡å¤ç¿»è¯‘æ—¶å¯ç”¨
"""

import os
import json
import argparse
import time
import re
import shutil
from typing import List, Dict, Any, Tuple
from tqdm import tqdm
import traceback
import concurrent.futures
import threading
from datetime import datetime

from utils.subtitle_parser import SubtitleParser, Subtitle
from utils.ai_service import AIService
from utils.vector_tools import VectorTools


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="AIé©±åŠ¨çš„æ™ºèƒ½å­—å¹•ç¿»è¯‘å·¥å…·")
    parser.add_argument("--input", "-i", required=True, help="è¾“å…¥å­—å¹•æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", "-o", help="è¾“å‡ºç¿»è¯‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--lang", "-l", default="zh-CN", help="ç›®æ ‡è¯­è¨€")
    parser.add_argument("--chunk-size", "-c", type=int, default=10, help="å¤„ç†å—å¤§å°")
    parser.add_argument("--vector-chunk-size", "-v", type=int, default=3, help="å‘é‡åŒ–å—å¤§å°")
    parser.add_argument("--save-analysis", "-s", action="store_true", help="ä¿å­˜å†…å®¹åˆ†æç»“æœ")
    parser.add_argument("--use-cache", "-u", action="store_true", help="ä½¿ç”¨ç¼“å­˜çš„å‘é‡æ•°æ®")
    parser.add_argument("--cache-dir", "-d", default=".cache", help="ç¼“å­˜ç›®å½•")
    parser.add_argument("--similar-chunks", "-k", type=int, default=2, help="æ¯ä¸ªå—ä½¿ç”¨çš„ç›¸ä¼¼å—æ•°é‡")
    parser.add_argument("--output-dir", type=str, default="translations", help="ç¿»è¯‘ç»“æœè¾“å‡ºç›®å½•")
    parser.add_argument("--workers", "-w", type=int, default=4, help="ç¿»è¯‘å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°é‡")
    parser.add_argument("--vectorize-workers", "-vw", type=int, default=4, help="å‘é‡åŒ–å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°é‡")
    parser.add_argument("--batch-delay", type=float, default=0.2, help="å¤šçº¿ç¨‹ç¿»è¯‘æ—¶æ¯æ‰¹æ¬¡å»¶è¿Ÿ(ç§’)ï¼Œé˜²æ­¢APIé™åˆ¶")
    parser.add_argument("--embedding-model", type=str, help="æŒ‡å®šåµŒå…¥æ¨¡å‹åç§°ï¼Œè¦†ç›–ç¯å¢ƒå˜é‡è®¾ç½®")
    parser.add_argument("--translation-model", type=str, help="æŒ‡å®šç¿»è¯‘æ¨¡å‹åç§°ï¼Œè¦†ç›–ç¯å¢ƒå˜é‡è®¾ç½®")
    parser.add_argument("--analysis-model", type=str, help="æŒ‡å®šåˆ†ææ¨¡å‹åç§°ï¼Œè¦†ç›–ç¯å¢ƒå˜é‡è®¾ç½®")
    parser.add_argument("--force-analysis", action="store_true", help="å¼ºåˆ¶é‡æ–°åˆ†æå†…å®¹")
    parser.add_argument("--align", action="store_true", help="å¯ç”¨å­—å¹•æ ¡å‡†åŠŸèƒ½ï¼Œç¡®ä¿ç¿»è¯‘å­—å¹•ä¸æºå­—å¹•ç²¾ç¡®å¯¹åº”å¹¶ä¼˜åŒ–æ’ç‰ˆ")
    
    return parser.parse_args()


def get_movie_name(file_path):
    """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–ç”µå½±åç§°"""
    base_name = os.path.basename(file_path)
    # ç§»é™¤æ‰©å±•å
    movie_name = os.path.splitext(base_name)[0]
    # ç§»é™¤å¸¸è§åç¼€å¦‚.engç­‰
    movie_name = re.sub(r'\.(eng|chs|cht|jp|kor)$', '', movie_name, flags=re.IGNORECASE)
    return movie_name


def ensure_dir(directory):
    """ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚ä¸å­˜åœ¨åˆ™åˆ›å»º"""
    if not os.path.exists(directory):
        os.makedirs(directory)
        print(f"åˆ›å»ºç›®å½•: {directory}")
    return directory


def translate_chunk(args):
    """
    ç¿»è¯‘å•ä¸ªå­—å¹•å—çš„å·¥ä½œå‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹å¤„ç†
    
    Args:
        args: åŒ…å«ç¿»è¯‘æ‰€éœ€å‚æ•°çš„å…ƒç»„
        
    Returns:
        ç¿»è¯‘ç»“æœå’Œå—ç´¢å¼•çš„å…ƒç»„
    """
    i, chunk, ai_service, chunk_embedding, chunk_embeddings, chunks, terminology_embeddings, content_analysis, args_obj, chunk_subtitles = args
    
    try:
        # æŸ¥æ‰¾ç›¸ä¼¼çš„å—
        similar_chunk_indices = []
        similar_chunks = []
        if chunk_embeddings and len(chunk_embeddings) > 0:
            similar_chunk_indices = VectorTools.static_find_similar_chunks(
                chunk_embedding, 
                chunk_embeddings,
                top_k=args_obj.similar_chunks
            )
            similar_chunks = [chunks[idx] for idx in similar_chunk_indices if idx < len(chunks)]
        
        # æå–ç›¸å…³æœ¯è¯­
        relevant_terms = []
        if terminology_embeddings and len(terminology_embeddings) > 0:
            relevant_terms = VectorTools.static_extract_key_terms(
                chunk_embedding,
                terminology_embeddings,
                threshold=0.75
            )
        
        # ä½¿ç”¨å¢å¼ºä¸Šä¸‹æ–‡çš„ç¿»è¯‘æ–¹æ³•
        translated_chunk = ai_service.translate_with_context(
            chunk, 
            target_language=args_obj.lang,
            content_analysis=content_analysis,
            similar_chunks=similar_chunks,
            relevant_terms=relevant_terms
        )
        
        # æ‹†åˆ†ç¿»è¯‘ç»“æœ
        lines = translated_chunk.strip().split("\n")
        chunk_translations = []
        
        # åŒ¹é…ç¿»è¯‘ç»“æœä¸åŸå§‹å­—å¹•
        current_sub_idx = 0
        current_line_group = []
        
        for line in lines:
            # è·³è¿‡ç©ºè¡Œ
            if not line.strip():
                if current_line_group:
                    chunk_translations.append("\n".join(current_line_group))
                    current_line_group = []
                    current_sub_idx += 1
                continue
            
            # è·³è¿‡å¯èƒ½çš„åºå·æˆ–æ—¶é—´ç è¡Œ
            if not re.match(r'^\d+\.?$', line) and not re.match(r'^\d{2}:\d{2}:\d{2}', line):
                current_line_group.append(line)
        
        # æ·»åŠ æœ€åä¸€ç»„
        if current_line_group:
            chunk_translations.append("\n".join(current_line_group))
        
        # ç¡®ä¿ç¿»è¯‘æ•°é‡ä¸å½“å‰å—ä¸­çš„å­—å¹•æ•°é‡åŒ¹é…
        expected_count = len(chunk_subtitles)
        
        if len(chunk_translations) != expected_count:
            # å°è¯•åŸºäºè¡Œæ•°åŒ¹é…
            if len(lines) >= expected_count:
                chunk_translations = []
                lines_per_subtitle = len(lines) // expected_count
                for j in range(expected_count):
                    start_idx = j * lines_per_subtitle
                    end_idx = start_idx + lines_per_subtitle
                    if j == expected_count - 1:  # æœ€åä¸€ä¸ªå­—å¹•
                        end_idx = len(lines)
                    sub_lines = lines[start_idx:end_idx]
                    chunk_translations.append("\n".join([l for l in sub_lines if l.strip()]))
            else:
                # å¦‚æœä¸åŒ¹é…ï¼Œåˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²å¡«å……æˆ–æˆªæ–­
                if len(chunk_translations) < expected_count:
                    chunk_translations.extend([""] * (expected_count - len(chunk_translations)))
                else:
                    chunk_translations = chunk_translations[:expected_count]
        
        return (i, chunk_translations)
    except Exception as e:
        print(f"âš ï¸ ç¿»è¯‘å— {i+1} æ—¶å‡ºé”™: {e}")
        # å¡«å……ç©ºç™½ç¿»è¯‘
        expected_count = len(chunk_subtitles)
        return (i, [""] * expected_count)


def translate_subtitles(subtitle_parser, ai_service, vector_tools, args, content_analysis=None, chunk_embeddings=None, chunks=None):
    """ç¿»è¯‘å­—å¹•å†…å®¹
    
    æ€§èƒ½ä¼˜åŒ–å»ºè®®:
    - å¯¹äº>500è¡Œçš„å­—å¹•ï¼Œå¯è€ƒè™‘å°†batch_sizeå¢åŠ åˆ°100ï¼Œæé«˜ååé‡
    - worker_countå¯æ ¹æ®ç³»ç»ŸCPUæ ¸å¿ƒæ•°å’Œç½‘ç»œçŠ¶å†µé€‚å½“å¢åŠ ï¼Œä½†æ³¨æ„APIé€Ÿç‡é™åˆ¶
    - å¦‚éœ€æè‡´é€Ÿåº¦ï¼Œå¯å°†batch_delayå‡å°‘è‡³0ï¼Œä½†éœ€ç¡®ä¿APIä¸ä¼šé™æµ
    - å¤§å‹å­—å¹•æ–‡ä»¶å¯å¢åŠ scene_boundary_thresholdçš„å€¼(>5ç§’)ä»¥å‡å°‘åœºæ™¯è¾¹ç•Œæ•°é‡
    - å°†similar_chunkæ£€æµ‹é¢‘ç‡ä»10è°ƒæ•´ä¸º20å¯è¿›ä¸€æ­¥å‡å°‘APIè°ƒç”¨
    
    Args:
        subtitle_parser: å­—å¹•è§£æå™¨
        ai_service: AIæœåŠ¡
        vector_tools: å‘é‡å·¥å…·
        args: å‘½ä»¤è¡Œå‚æ•°
        content_analysis: å†…å®¹åˆ†æç»“æœ
        chunk_embeddings: å—åµŒå…¥å‘é‡
        chunks: æ–‡æœ¬å—
        
    Returns:
        ç¿»è¯‘åçš„å­—å¹•æ–‡æœ¬
    """
    print(f"ğŸ”  å¼€å§‹ç¿»è¯‘å­—å¹•åˆ° {args.lang}... (70%)")
    
    # è·å–æ‰€æœ‰å­—å¹•
    subtitles = subtitle_parser.subtitles
    total = len(subtitles)
    
    # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°è°ƒæ•´å·¥ä½œçº¿ç¨‹æ•°å’Œå»¶è¿Ÿ
    # æé«˜æ€§èƒ½ï¼šä½¿ç”¨æ›´å¤šçº¿ç¨‹ï¼Œæ›´å¤§æ‰¹æ¬¡ï¼Œæ›´å°å»¶è¿Ÿ
    worker_count = min(args.workers, 20)  # çº¿ç¨‹æ•°å¢åŠ åˆ°20ï¼Œå¤§å¹…å¢åŠ å¹¶å‘
    batch_delay = max(args.batch_delay, 0.0)  # å‡å°‘æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œä¾èµ–APIè‡ªå·±çš„é™æµ
    batch_size = 50  # æ˜¾è‘—å¢å¤§æ‰¹å¤„ç†é‡ï¼Œå‡å°‘å¾ªç¯æ¬¡æ•°
    
    # ä¼˜åŒ–ï¼šé¢„å¤„ç†å…³é”®å¸§å’Œåœºæ™¯è½¬æ¢ç‚¹ï¼Œæé«˜ä¸Šä¸‹æ–‡ç†è§£
    # è¯†åˆ«å¯èƒ½çš„åœºæ™¯è¾¹ç•Œ
    scene_boundaries = []
    prev_time = None
    for i, subtitle in enumerate(subtitles):
        if prev_time is not None:
            # è·å–å½“å‰å­—å¹•çš„å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
            start_parts = str(subtitle.start).split(':')
            start_seconds = int(start_parts[0]) * 3600 + int(start_parts[1]) * 60 + float(start_parts[2].replace(',', '.'))
            
            # è·å–å‰ä¸€å­—å¹•çš„ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
            end_parts = prev_time.split(':')
            end_seconds = int(end_parts[0]) * 3600 + int(end_parts[1]) * 60 + float(end_parts[2].replace(',', '.'))
            
            # å¦‚æœé—´éš”è¶…è¿‡5ç§’ï¼Œå¯èƒ½æ˜¯åœºæ™¯è½¬æ¢
            if start_seconds - end_seconds > 5:
                scene_boundaries.append(i)
        
        prev_time = str(subtitle.end)
    
    print(f"ğŸ“Š æ£€æµ‹åˆ° {len(scene_boundaries)} ä¸ªå¯èƒ½çš„åœºæ™¯è½¬æ¢ç‚¹")
    
    # æ˜¯å¦ä½¿ç”¨å¤šçº¿ç¨‹ç¿»è¯‘
    if worker_count > 1:
        # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—
        tasks = []
        
        # æ¯ä¸ªå­—å¹•éƒ½æ˜¯ä¸€ä¸ªç¿»è¯‘ä»»åŠ¡
        for i, subtitle in enumerate(subtitles):
            original_text = subtitle.text
            
            # è·³è¿‡ç©ºå­—å¹•
            if not original_text.strip():
                continue
                
            # è·å–ç›¸ä¼¼å—ï¼ˆä¼˜åŒ–ï¼šåŒæ—¶è€ƒè™‘å‰åç›¸é‚»å­—å¹•ä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
            similar_chunks = []
            
            # æ·»åŠ ç›¸é‚»å­—å¹•ä½œä¸ºç›´æ¥ä¸Šä¸‹æ–‡
            for j in range(max(0, i-3), min(len(subtitles), i+4)):
                if j != i and subtitles[j].text.strip():
                    similar_chunks.append(subtitles[j].text)
            
            # è·å–è¯­ä¹‰ç›¸ä¼¼çš„å—
            try:
                # æ¯éš”10ä¸ªå­—å¹•è®¡ç®—ä¸€æ¬¡å‘é‡ç›¸ä¼¼åº¦ï¼ˆåŸæ¥æ˜¯20ï¼‰
                # å‡å°‘ç›¸ä¼¼åº¦è®¡ç®—é¢‘ç‡ä½†å¢åŠ ç›¸é‚»å­—å¹•çš„ç›´æ¥ä½¿ç”¨
                if i % 10 == 0 and chunk_embeddings:
                    text_embedding = ai_service.get_embedding(original_text)
                    similar_indices = vector_tools.find_similar_chunks(text_embedding, chunk_embeddings, args.similar_chunks)
                    for idx in similar_indices:
                        if chunks and idx < len(chunks):
                            chunk_text = " ".join(chunks[idx])
                            if chunk_text not in similar_chunks:  # é¿å…é‡å¤
                                similar_chunks.append(chunk_text)
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: è·å–ç›¸ä¼¼å—å¤±è´¥: {e}")
            
            # å¢åŠ åœºæ™¯ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡è¾…åŠ©
            is_scene_boundary = i in scene_boundaries
            
            tasks.append({
                "index": i,
                "subtitle": subtitle,
                "text": original_text,
                "similar_chunks": similar_chunks,
                "is_scene_boundary": is_scene_boundary
            })
        
        # ä¼˜åŒ–ï¼šå¯¹å­—å¹•æ–‡æœ¬è¿›è¡Œåˆ†ç»„ï¼Œæ³¨æ„ä¿æŒåœºæ™¯è¿è´¯æ€§
        # å°†è¿ç»­çš„çŸ­å­—å¹•ç»„åˆèµ·æ¥ä¸€æ¬¡æ€§ç¿»è¯‘ï¼Œæé«˜æ•ˆç‡
        combined_tasks = []
        temp_group = []
        temp_text = ""
        temp_indices = []
        
        for task in tasks:
            # å¦‚æœæ˜¯åœºæ™¯è¾¹ç•Œï¼Œå…ˆç»“æŸå½“å‰ç»„
            if task["is_scene_boundary"] and temp_group:
                combined_tasks.append({
                    "indices": temp_indices,
                    "text": temp_text.strip(),
                    "similar_chunks": temp_group[0]["similar_chunks"],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„ç›¸ä¼¼å—
                    "subtitles": [t["subtitle"] for t in temp_group]
                })
                temp_group = []
                temp_text = ""
                temp_indices = []
            
            # å¦‚æœå½“å‰æ–‡æœ¬é•¿åº¦åŠ ä¸Šæ–°ä»»åŠ¡ä¸è¶…è¿‡2000å­—ç¬¦ï¼Œåˆ™ç»„åˆ
            # ä½†å•ä¸ªç»„æœ€å¤šä¸è¶…è¿‡5ä¸ªå­—å¹•ï¼Œé¿å…è¿‡åº¦ç»„åˆå¯¼è‡´ç¿»è¯‘è´¨é‡ä¸‹é™
            if len(temp_text) + len(task["text"]) < 2000 and len(temp_group) < 5:
                temp_group.append(task)
                temp_text += "\n\n" + task["text"]
                temp_indices.append(task["index"])
            else:
                # å¦‚æœç»„é‡Œæœ‰å†…å®¹ï¼Œå…ˆä¿å­˜å½“å‰ç»„
                if temp_group:
                    combined_tasks.append({
                        "indices": temp_indices,
                        "text": temp_text.strip(),
                        "similar_chunks": temp_group[0]["similar_chunks"],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„ç›¸ä¼¼å—
                        "subtitles": [t["subtitle"] for t in temp_group]
                    })
                # é‡æ–°å¼€å§‹æ–°çš„ç»„
                temp_group = [task]
                temp_text = task["text"]
                temp_indices = [task["index"]]
        
        # æ·»åŠ æœ€åä¸€ç»„
        if temp_group:
            combined_tasks.append({
                "indices": temp_indices,
                "text": temp_text.strip(),
                "similar_chunks": temp_group[0]["similar_chunks"],
                "subtitles": [t["subtitle"] for t in temp_group]
            })
        
        print(f"ğŸ“Š ä¼˜åŒ–ï¼šå°† {len(tasks)} æ¡å­—å¹•ç»„åˆä¸º {len(combined_tasks)} ä¸ªç¿»è¯‘ä»»åŠ¡")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œç¿»è¯‘
        with concurrent.futures.ThreadPoolExecutor(max_workers=worker_count) as executor:
            results = {}
            
            print(f"ğŸ§µ ä½¿ç”¨ {worker_count} ä¸ªå·¥ä½œçº¿ç¨‹è¿›è¡Œç¿»è¯‘ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}, å»¶è¿Ÿ: {batch_delay}ç§’ï¼‰...")
            print(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {ai_service.translation_model}")
            
            with tqdm(total=total, desc="ç¿»è¯‘è¿›åº¦") as pbar:
                # åˆ†æ‰¹æ¬¡å¤„ç†
                for i in range(0, len(combined_tasks), batch_size):
                    batch = combined_tasks[i:i+batch_size]
                    
                    # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                    futures = []
                    for task in batch:
                        future = executor.submit(
                            ai_service.translate_subtitle_chunk,
                            task["text"],
                            args.lang,
                            content_analysis,
                            task["similar_chunks"]
                        )
                        futures.append((future, task["indices"], task["subtitles"]))
                    
                    # æ”¶é›†ç»“æœ
                    for future, indices, batch_subtitles in futures:
                        try:
                            translation = future.result()
                            
                            if translation:
                                # æ‹†åˆ†ç¿»è¯‘ç»“æœå›å¤šä¸ªå­—å¹•
                                if len(indices) > 1:
                                    # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ‹†åˆ†æ–¹æ³•
                                    # é¦–å…ˆå°è¯•æŒ‰ç…§ç©ºè¡Œæ‹†åˆ†
                                    parts = translation.split("\n\n")
                                    
                                    # å¦‚æœæ‹†åˆ†å‡ºçš„éƒ¨åˆ†ä¸åŸå­—å¹•æ•°é‡åŒ¹é…
                                    if len(parts) == len(indices):
                                        for j, idx in enumerate(indices):
                                            results[idx] = parts[j]
                                    else:
                                        # å°è¯•æŒ‰å¥å­æ‹†åˆ†
                                        sentences = re.split(r'([ã€‚ï¼ï¼Ÿâ€¦])', translation)
                                        # åˆå¹¶å¥å­ä¸æ ‡ç‚¹
                                        real_sentences = []
                                        for k in range(0, len(sentences)-1, 2):
                                            if k+1 < len(sentences):
                                                real_sentences.append(sentences[k] + sentences[k+1])
                                            else:
                                                real_sentences.append(sentences[k])
                                        
                                        # å¦‚æœå¥å­æ•°é‡ä¸å­—å¹•æ•°é‡åŒ¹é…æˆ–æ¥è¿‘
                                        if abs(len(real_sentences) - len(indices)) <= 1:
                                            # å°½é‡å‡åŒ€åˆ†é…å¥å­åˆ°å­—å¹•
                                            sentences_per_subtitle = max(1, len(real_sentences) // len(indices))
                                            for j, idx in enumerate(indices):
                                                start = j * sentences_per_subtitle
                                                end = min(start + sentences_per_subtitle, len(real_sentences))
                                                if j == len(indices) - 1:  # æœ€åä¸€ä¸ªå­—å¹•æ‹¿å‰©ä¸‹æ‰€æœ‰å¥å­
                                                    end = len(real_sentences)
                                                if start < len(real_sentences):
                                                    results[idx] = "".join(real_sentences[start:end])
                                                else:
                                                    results[idx] = ""
                                        else:
                                            # æŒ‰æ¯”ä¾‹æ‹†åˆ†æ–‡æœ¬
                                            total_chars = len(translation)
                                            total_original_chars = sum([len(sub.text) for sub in batch_subtitles])
                                            
                                            start_pos = 0
                                            for j, idx in enumerate(indices):
                                                # æŒ‰åŸå§‹æ–‡æœ¬é•¿åº¦æ¯”ä¾‹åˆ†é…ç¿»è¯‘ç»“æœ
                                                original_ratio = len(batch_subtitles[j].text) / total_original_chars
                                                char_count = int(total_chars * original_ratio)
                                                
                                                # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                                                end_pos = min(start_pos + char_count, total_chars)
                                                
                                                # æå–å¯¹åº”çš„ç¿»è¯‘ç‰‡æ®µ
                                                results[idx] = translation[start_pos:end_pos].strip()
                                                start_pos = end_pos
                                else:
                                    # å•ä¸ªå­—å¹•ç›´æ¥ä½¿ç”¨æ•´ä¸ªç¿»è¯‘ç»“æœ
                                    results[indices[0]] = translation
                            else:
                                # å¦‚æœç¿»è¯‘ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸæ–‡
                                for idx in indices:
                                    results[idx] = tasks[idx]["text"] if idx < len(tasks) else ""
                                    
                            # æ›´æ–°è¿›åº¦æ¡
                            pbar.update(len(indices))
                            pbar.set_description(f"ç¿»è¯‘è¿›åº¦ ({int(pbar.n/pbar.total*100)}%)")
                        except Exception as e:
                            print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
                            # å¤±è´¥æ—¶ä½¿ç”¨åŸæ–‡
                            for idx in indices:
                                if idx < len(tasks):
                                    results[idx] = tasks[idx]["text"]
                            pbar.update(len(indices))
                    
                    # æ¯æ‰¹æ¬¡ä¹‹é—´æ·»åŠ æçŸ­å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                    if i + batch_size < len(combined_tasks) and batch_delay > 0:
                        time.sleep(batch_delay)
            
            # æ›´æ–°å­—å¹•ç¿»è¯‘ç»“æœ
            for idx, translation in results.items():
                if idx < len(subtitles):
                    subtitles[idx].text = translation
    else:
        # å•çº¿ç¨‹ç¿»è¯‘ - ä½†ä¼˜åŒ–å¤„ç†æ–¹å¼
        print("ä½¿ç”¨å•çº¿ç¨‹ç¿»è¯‘...")
        print(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {ai_service.translation_model}")
        
        # å°†å­—å¹•åˆ†ç»„ä»¥å‡å°‘APIè°ƒç”¨ï¼Œä½†è€ƒè™‘åœºæ™¯è¾¹ç•Œ
        groups = []
        current_group = []
        current_text = ""
        
        for i, subtitle in enumerate(subtitles):
            if not subtitle.text.strip():
                continue
            
            # å¦‚æœæ˜¯åœºæ™¯è¾¹ç•Œï¼Œå…ˆç»“æŸå½“å‰ç»„
            if i in scene_boundaries and current_group:
                groups.append(current_group)
                current_group = []
                current_text = ""
                
            # å¦‚æœä¸è¶…è¿‡2000å­—ç¬¦ä¸”ä¸è¶…è¿‡5ä¸ªå­—å¹•ï¼Œæ·»åŠ åˆ°å½“å‰ç»„
            if len(current_text) + len(subtitle.text) < 2000 and len(current_group) < 5:
                # æ”¶é›†ç›¸é‚»å­—å¹•ä½œä¸ºä¸Šä¸‹æ–‡
                similar_chunks = []
                for j in range(max(0, i-3), min(len(subtitles), i+4)):
                    if j != i and subtitles[j].text.strip():
                        similar_chunks.append(subtitles[j].text)
                
                current_group.append((i, subtitle, similar_chunks))
                current_text += "\n\n" + subtitle.text
            else:
                # ä¿å­˜å½“å‰ç»„å¹¶å¼€å§‹æ–°ç»„
                if current_group:
                    groups.append(current_group)
                
                # æ”¶é›†ç›¸é‚»å­—å¹•ä½œä¸ºä¸Šä¸‹æ–‡
                similar_chunks = []
                for j in range(max(0, i-3), min(len(subtitles), i+4)):
                    if j != i and subtitles[j].text.strip():
                        similar_chunks.append(subtitles[j].text)
                
                current_group = [(i, subtitle, similar_chunks)]
                current_text = subtitle.text
        
        # æ·»åŠ æœ€åä¸€ç»„
        if current_group:
            groups.append(current_group)
            
        print(f"ğŸ“Š ä¼˜åŒ–ï¼šå°† {len(subtitles)} æ¡å­—å¹•ç»„åˆä¸º {len(groups)} ä¸ªç¿»è¯‘ä»»åŠ¡")
        
        with tqdm(total=total, desc="ç¿»è¯‘è¿›åº¦") as pbar:
            for group in groups:
                # ç»„åˆæ–‡æœ¬
                combined_text = "\n\n".join([sub.text for _, sub, _ in group])
                
                # è·å–æ‰€æœ‰ç›¸ä¼¼å—å¹¶åˆå¹¶å»é‡
                all_similar_chunks = []
                for _, _, similar_chunks in group:
                    for chunk in similar_chunks:
                        if chunk not in all_similar_chunks:
                            all_similar_chunks.append(chunk)
                
                # é™åˆ¶ç›¸ä¼¼å—æ•°é‡ï¼Œä¼˜å…ˆä¿ç•™å‰åç›¸é‚»çš„
                similar_chunks = all_similar_chunks[:6]
                
                # è·å–å‘é‡ç›¸ä¼¼çš„å—
                try:
                    # ä¸ºæ¯ç»„è®¡ç®—ä¸€æ¬¡ç›¸ä¼¼åº¦
                    if chunk_embeddings:
                        text_embedding = ai_service.get_embedding(combined_text[:1000])  # é™åˆ¶é•¿åº¦
                        similar_indices = vector_tools.find_similar_chunks(text_embedding, chunk_embeddings, args.similar_chunks)
                        for idx in similar_indices:
                            if chunks and idx < len(chunks):
                                chunk_text = " ".join(chunks[idx])
                                if chunk_text not in similar_chunks:  # é¿å…é‡å¤
                                    similar_chunks.append(chunk_text)
                except Exception as e:
                    print(f"âš ï¸ è­¦å‘Š: è·å–ç›¸ä¼¼å—å¤±è´¥: {e}")
                
                # ç¿»è¯‘ç»„åˆæ–‡æœ¬
                try:
                    translation = ai_service.translate_subtitle_chunk(
                        combined_text,
                        args.lang,
                        content_analysis,
                        similar_chunks
                    )
                    
                    if translation:
                        # æ‹†åˆ†ç¿»è¯‘ç»“æœ
                        if len(group) > 1:
                            # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ‹†åˆ†æ–¹æ³•
                            parts = translation.split("\n\n")
                            
                            # å¦‚æœæ‹†åˆ†éƒ¨åˆ†ä¸å­—å¹•æ•°é‡åŒ¹é…
                            if len(parts) == len(group):
                                for j, (idx, sub, _) in enumerate(group):
                                    sub.text = parts[j]
                            else:
                                # å°è¯•æŒ‰å¥å­æ‹†åˆ†
                                sentences = re.split(r'([ã€‚ï¼ï¼Ÿâ€¦])', translation)
                                # åˆå¹¶å¥å­ä¸æ ‡ç‚¹
                                real_sentences = []
                                for k in range(0, len(sentences)-1, 2):
                                    if k+1 < len(sentences):
                                        real_sentences.append(sentences[k] + sentences[k+1])
                                    else:
                                        real_sentences.append(sentences[k])
                                
                                # å¦‚æœå¥å­æ•°é‡ä¸å­—å¹•æ•°é‡åŒ¹é…æˆ–æ¥è¿‘
                                if abs(len(real_sentences) - len(group)) <= 1:
                                    # å°½é‡å‡åŒ€åˆ†é…å¥å­åˆ°å­—å¹•
                                    sentences_per_subtitle = max(1, len(real_sentences) // len(group))
                                    for j, (_, sub, _) in enumerate(group):
                                        start = j * sentences_per_subtitle
                                        end = min(start + sentences_per_subtitle, len(real_sentences))
                                        if j == len(group) - 1:  # æœ€åä¸€ä¸ªå­—å¹•æ‹¿å‰©ä¸‹æ‰€æœ‰å¥å­
                                            end = len(real_sentences)
                                        if start < len(real_sentences):
                                            sub.text = "".join(real_sentences[start:end])
                                else:
                                    # æŒ‰æ¯”ä¾‹åˆ†é…ç¿»è¯‘ç»“æœ
                                    total_chars = len(translation)
                                    total_original_chars = sum([len(sub.text) for _, sub, _ in group])
                                    
                                    start_pos = 0
                                    for _, sub, _ in group:
                                        original_ratio = len(sub.text) / total_original_chars
                                        char_count = int(total_chars * original_ratio)
                                        
                                        end_pos = min(start_pos + char_count, total_chars)
                                        sub.text = translation[start_pos:end_pos].strip()
                                        start_pos = end_pos
                        else:
                            # å•ä¸ªå­—å¹•ç›´æ¥ä½¿ç”¨æ•´ä¸ªç¿»è¯‘
                            group[0][1].text = translation
                except Exception as e:
                    print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.update(len(group))
                pbar.set_description(f"ç¿»è¯‘è¿›åº¦ ({int(pbar.n/pbar.total*100)}%)")
    
    # è¿”å›ç¿»è¯‘åçš„å­—å¹•è§£æå™¨
    return subtitle_parser


def save_srt(subtitle_parser, output_path, encoding='utf-8'):
    """æ­£ç¡®æ ¼å¼ä¿å­˜SRTå­—å¹•æ–‡ä»¶ï¼Œé¿å…æ ¼å¼é—®é¢˜
    
    Args:
        subtitle_parser: å­—å¹•è§£æå™¨å¯¹è±¡
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        encoding: æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤UTF-8
    """
    with open(output_path, 'w', encoding=encoding) as f:
        for i, item in enumerate(subtitle_parser.subtitles):
            # ç¡®ä¿ç´¢å¼•æ˜¯è¿ç»­çš„
            item.index = i + 1
            
            # æ ¼å¼åŒ–æ—¶é—´ç è¡Œ
            timestamp_line = f"{item.start} --> {item.end}"
            
            # ç¡®ä¿æ–‡æœ¬æ²¡æœ‰å‰åå¤šä½™çš„ç©ºè¡Œ
            text = item.text.strip()
            
            # å†™å…¥æ ‡å‡†æ ¼å¼çš„å­—å¹•æ¡ç›®
            f.write(f"{item.index}\n")
            f.write(f"{timestamp_line}\n")
            f.write(f"{text}\n")
            
            # åœ¨æ¯ä¸ªå­—å¹•æ¡ç›®ä¹‹é—´åªæ·»åŠ ä¸€ä¸ªç©ºè¡Œ
            if i < len(subtitle_parser.subtitles) - 1:
                f.write("\n")


def main():
    """ä¸»ç¨‹åº"""
    parser = argparse.ArgumentParser(description='å­—å¹•ç¿»è¯‘å·¥å…·')
    parser.add_argument('--input', required=True, help='è¾“å…¥å­—å¹•æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºç¿»è¯‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--lang', default='zh-CN', help='ç›®æ ‡è¯­è¨€ï¼Œé»˜è®¤ä¸­æ–‡')
    parser.add_argument('--chunk-size', type=int, default=3, help='åˆ†å—å¤§å°')
    parser.add_argument('--vector-chunk-size', type=int, default=5, help='å‘é‡åŒ–åˆ†å—å¤§å°')
    parser.add_argument('--save-analysis', action='store_true', help='æ˜¯å¦ä¿å­˜åˆ†æç»“æœ')
    parser.add_argument('--force-analysis', action='store_true', help='å¼ºåˆ¶é‡æ–°åˆ†æå†…å®¹')
    parser.add_argument('--use-cache', action='store_true', help='ä½¿ç”¨ç¼“å­˜åŠ é€Ÿ')
    parser.add_argument('--cache-dir', help='ç¼“å­˜ç›®å½•')
    parser.add_argument('--similar-chunks', type=int, default=5, help='åŒ¹é…ç›¸ä¼¼å—æ•°é‡')
    parser.add_argument('--output-dir', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--workers', type=int, default=1, help='ç¿»è¯‘å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--vectorize-workers', type=int, default=2, help='å‘é‡åŒ–å·¥ä½œçº¿ç¨‹æ•°')
    parser.add_argument('--batch-delay', type=float, default=1.0, help='æ‰¹å¤„ç†å»¶è¿Ÿ(ç§’)')
    parser.add_argument('--embedding-model', help='æŒ‡å®šåµŒå…¥æ¨¡å‹åç§°')
    parser.add_argument('--translation-model', help='æŒ‡å®šç¿»è¯‘æ¨¡å‹åç§°')
    parser.add_argument('--analysis-model', help='æŒ‡å®šåˆ†ææ¨¡å‹åç§°')
    parser.add_argument('--align', action='store_true', help='å¯ç”¨å­—å¹•æ ¡å‡†åŠŸèƒ½ï¼Œç¡®ä¿ç¿»è¯‘å­—å¹•ä¸æºå­—å¹•ç²¾ç¡®å¯¹åº”å¹¶ä¼˜åŒ–æ’ç‰ˆ')

    args = parser.parse_args()

    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(args.input):
            print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{args.input}' ä¸å­˜åœ¨")
            return

        # è·å–ç”µå½±åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = os.path.basename(args.input)
        movie_name = os.path.splitext(base_name)[0]
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if args.output_dir:
            output_dir = args.output_dir
        else:
            output_dir = os.path.join("translations", movie_name)
            
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if not os.path.exists(output_dir):
            print(f"åˆ›å»ºç›®å½•: {output_dir}")
            os.makedirs(output_dir, exist_ok=True)
            
        print(f"ğŸ¬ æ­£åœ¨å¤„ç†å­—å¹•æ–‡ä»¶: {args.input}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
        
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
        if args.output:
            output_file = args.output
        else:
            output_file = os.path.join(output_dir, f"{movie_name}.{args.lang}.srt")
            
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_dir = args.cache_dir if args.cache_dir else os.path.join(".cache", movie_name)
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–è¿›åº¦
        progress = 0
        print(f"âš™ï¸ åˆå§‹åŒ–æœåŠ¡... ({progress}%)")
        
        # åˆå§‹åŒ–å­—å¹•è§£æå™¨
        subtitle_parser = SubtitleParser(args.input)
        
        # åˆå§‹åŒ–AIæœåŠ¡
        ai_service = AIService(
            embedding_model=args.embedding_model,
            translation_model=args.translation_model,
            analysis_model=args.analysis_model
        )
        
        # åˆå§‹åŒ–å‘é‡å·¥å…·
        vector_tools = VectorTools()
        
        print(f"ğŸ¤– ä½¿ç”¨åµŒå…¥æ¨¡å‹: {ai_service.embedding_model}")
        print(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {ai_service.translation_model}")
        print(f"ğŸ¤– ä½¿ç”¨åˆ†ææ¨¡å‹: {ai_service.analysis_model}")
        
        # è·å–å­—å¹•å…ƒæ•°æ®
        metadata = subtitle_parser.get_metadata()
        print(f"â„¹ï¸ å­—å¹•ä¿¡æ¯: {metadata['subtitle_count']} æ¡å­—å¹•, æ€»æ—¶é•¿: {metadata['duration']}")
        
        progress = 5
        print(f"âš™ï¸ åˆå§‹åŒ–å®Œæˆ ({progress}%)")
        
        # 1. åˆ†æå­—å¹•æ–‡æœ¬
        progress = 10
        print(f"ğŸ” æ­£åœ¨åˆ†æå­—å¹•å†…å®¹... ({progress}%)")
        
        subtitle_text = subtitle_parser.get_all_text()
        
        # ç”Ÿæˆå­—å¹•æ–‡æœ¬çš„åµŒå…¥å‘é‡
        progress = 15
        print(f"ğŸ§  æ­£åœ¨ç”Ÿæˆæ–‡æœ¬åµŒå…¥å‘é‡... ({progress}%)")
        print(f"ğŸ¤– ä½¿ç”¨åµŒå…¥æ¨¡å‹: {ai_service.embedding_model}")
        
        # è·å–å…¨æ–‡çš„åµŒå…¥å‘é‡
        full_embedding = ai_service.get_embedding(subtitle_text)
        
        progress = 20
        if len(full_embedding) > 0:
            embedding_dim = len(full_embedding)
            print(f"âœ… æˆåŠŸç”ŸæˆåµŒå…¥å‘é‡ï¼Œç»´åº¦: {embedding_dim} ({progress}%)")
        
        # ä½¿ç”¨å‘é‡å·¥å…·è¿›è¡Œå­—å¹•åˆ†å—å’Œå‘é‡åŒ–
        progress = 22
        print(f"ğŸ“Š å°†å­—å¹•åˆ†å—å¹¶å‘é‡åŒ– (å—å¤§å°: {args.chunk_size})... ({progress}%)")
        
        # å°†å­—å¹•åˆ†å—å¹¶è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
        chunks_data = vector_tools.chunk_and_vectorize(
            subtitle_parser.get_subtitle_chunks(args.chunk_size),
            ai_service.get_embedding,
            chunk_size=args.chunk_size,
            max_workers=args.vectorize_workers
        )
        
        chunks = chunks_data["chunks"]
        chunk_embeddings = chunks_data["embeddings"]
        chunk_count = chunks_data["total_chunks"]
        
        progress = 25
        print(f"ğŸ’¾ å‘é‡æ•°æ®å·²ç¼“å­˜ï¼Œå…± {chunk_count} ä¸ªå— ({progress}%)")
        
        # å¦‚æœæ²¡æœ‰ä»»ä½•å—ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•
        if chunk_count == 0:
            print("âš ï¸ è­¦å‘Š: å‘é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•...")
            chunks = [subtitle_text]
            if full_embedding:
                chunk_embeddings = [full_embedding]
            else:
                chunk_embeddings = []
                full_embedding = []
        
        # 2. åˆ†æå†…å®¹ç±»å‹
        content_analysis = None
        analysis_file = os.path.join(output_dir, f"{movie_name}.analysis.json")
        
        # å¦‚æœå­˜åœ¨åˆ†ææ–‡ä»¶ä¸”ä¸å¼ºåˆ¶é‡æ–°åˆ†æï¼Œåˆ™è¯»å–
        if os.path.exists(analysis_file) and not args.force_analysis:
            try:
                with open(analysis_file, 'r', encoding='utf-8') as f:
                    content_analysis = json.load(f)
                print(f"ğŸ“Š ä½¿ç”¨ç°æœ‰å†…å®¹åˆ†æç»“æœ (40%)")
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: è¯»å–åˆ†ææ–‡ä»¶å¤±è´¥: {e}")
                content_analysis = None
        
        # å¦‚æœæ²¡æœ‰å†…å®¹åˆ†æï¼Œåˆ™è¿›è¡Œåˆ†æ
        if not content_analysis:
            try:
                progress = 30
                print(f"ğŸ“ æ­£åœ¨åˆ†æå½±ç‰‡ç±»å‹å’Œå†…å®¹... ({progress}%)")
                print(f"ğŸ¤– ä½¿ç”¨åˆ†ææ¨¡å‹: {ai_service.analysis_model}")
                
                content_analysis = ai_service.analyze_content(subtitle_text)
                
                # ä¿å­˜åˆ†æç»“æœ
                if args.save_analysis:
                    with open(analysis_file, 'w', encoding='utf-8') as f:
                        json.dump(content_analysis, f, ensure_ascii=False, indent=2)
                    print(f"ğŸ“Š å†…å®¹åˆ†æç»“æœå·²ä¿å­˜è‡³ {analysis_file} (40%)")
                
                # æ‰“å°åˆ†æç»“æœæ‘˜è¦
                print("å½±ç‰‡åˆ†æç»“æœ:")
                if content_analysis and "genre" in content_analysis:
                    print(f"ğŸ­ å½±ç‰‡ç±»å‹: {content_analysis['genre']}")
                if content_analysis and "plot_summary" in content_analysis:
                    print(f"ğŸ“– å‰§æƒ…æ‘˜è¦: {content_analysis['plot_summary']}")
                    
                progress = 50
                print(f"âœ… å†…å®¹åˆ†æå®Œæˆ ({progress}%)")
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Š: å†…å®¹åˆ†æå¤±è´¥: {e}")
                progress = 50
        else:
            progress = 50
        
        # 3. æå–å…³é”®æœ¯è¯­
        progress = 60
        print(f"ğŸ”‘ æå–å…³é”®æœ¯è¯­... ({progress}%)")
        
        # ä½¿ç”¨åˆ†æç»“æœä¸­çš„æœ¯è¯­æˆ–ä»æ–‡æœ¬ä¸­æå–
        if content_analysis and "terminology" in content_analysis:
            # å¦‚æœåˆ†æç»“æœä¸­æœ‰æœ¯è¯­ï¼Œç›´æ¥ä½¿ç”¨
            if isinstance(content_analysis["terminology"], dict):
                terms = list(content_analysis["terminology"].keys())
            else:
                terms = [term.split(":")[0].strip() for term in content_analysis["terminology"] if ":" in term]
                if not terms:
                    terms = content_analysis["terminology"]
        else:
            # å¦åˆ™å°è¯•ä»æ–‡æœ¬ä¸­æå–
            terms = ai_service.extract_key_terms(subtitle_text)
            
        print(f"å‘ç° {len(terms)} ä¸ªå…³é”®æœ¯è¯­")
        
        # 4. å¼€å§‹ç¿»è¯‘å­—å¹•
        progress = 70
        print(f"ğŸ”  å¼€å§‹ç¿»è¯‘å­—å¹•åˆ° {args.lang}... ({progress}%)")
        
        # æ£€æµ‹åœºæ™¯è¾¹ç•Œ
        scene_boundaries = vector_tools.find_scene_boundaries(chunk_embeddings, threshold=0.7)
        print(f"ğŸ“Š æ£€æµ‹åˆ° {len(scene_boundaries)} ä¸ªå¯èƒ½çš„åœºæ™¯è½¬æ¢ç‚¹")
        
        # è¯†åˆ«ç›¸ä¼¼åœºæ™¯ä»¥å¢å¼ºç¿»è¯‘ä¸€è‡´æ€§
        similar_scenes = vector_tools.find_similar_scenes(
            chunk_embeddings, 
            scene_boundaries, 
            similarity_threshold=0.75
        )
        
        # å‡†å¤‡ç¿»è¯‘ä»»åŠ¡
        subtitles = subtitle_parser.subtitles
        tasks = []
        
        # åˆ›å»ºå­—å¹•ç´¢å¼•åˆ°å—ç´¢å¼•çš„æ˜ å°„
        subtitle_to_chunk_mapping = {}
        
        # æ„å»ºç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
        for i, subtitle in enumerate(subtitles):
            # è·å–å—ç´¢å¼•
            chunk_idx = i // args.chunk_size
            subtitle_to_chunk_mapping[i] = chunk_idx
            
            # è·å–ç›¸ä¼¼å—
            similar_chunk_indices = []
            if chunk_idx < len(chunks):
                similar_chunk_indices = vector_tools.find_similar_chunks(
                    chunk_embeddings[chunk_idx], 
                    chunk_embeddings, 
                    args.similar_chunks
                )
            
            similar_text = []
            for idx in similar_chunk_indices:
                if idx < len(chunks):
                    similar_text.append(" ".join(chunks[idx]))
            
            # åˆ›å»ºä»»åŠ¡
            tasks.append({
                "index": i,
                "text": subtitle.text,
                "similar_chunks": similar_text,
                "subtitle": subtitle
            })
        
        progress = 70
        print(f"ğŸ”  å¼€å§‹ç¿»è¯‘å­—å¹•åˆ° {args.lang}... ({progress}%)")
        
        # å†æ¬¡æ£€æµ‹åœºæ™¯è¾¹ç•Œä½†ä½¿ç”¨æ›´ä¿å®ˆçš„é˜ˆå€¼ï¼Œç¡®ä¿æ›´é«˜è´¨é‡çš„åˆ†ç»„
        conservative_boundaries = vector_tools.find_scene_boundaries(chunk_embeddings, threshold=0.5)
        print(f"ğŸ“Š æ£€æµ‹åˆ° {len(conservative_boundaries)} ä¸ªå¯èƒ½çš„åœºæ™¯è½¬æ¢ç‚¹")
        
        # ä¼˜åŒ–ï¼šæŒ‰åœºæ™¯è¾¹ç•Œç»„åˆä»»åŠ¡ä»¥å‡å°‘APIè°ƒç”¨
        combined_tasks = []
        temp_group = []
        temp_text = ""
        temp_indices = []
        
        # åœºæ™¯è¾¹ç•Œé›†åˆï¼Œç”¨äºåˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å‰²
        boundary_set = set()
        for b in conservative_boundaries:
            # å°†å—ç´¢å¼•è½¬æ¢ä¸ºå­—å¹•ç´¢å¼•èŒƒå›´
            start_subtitle = max(0, b * args.chunk_size - 1)
            end_subtitle = min(len(subtitles), (b + 1) * args.chunk_size + 1)
            for idx in range(start_subtitle, end_subtitle):
                boundary_set.add(idx)
        
        # ç»„åˆä»»åŠ¡
        for i, task in enumerate(tasks):
            # å¦‚æœå½“å‰å­—å¹•ç´¢å¼•æ˜¯åœºæ™¯è¾¹ç•Œï¼Œæˆ–è¾¾åˆ°æœ€å¤§ç»„å¤§å°ï¼Œå¼€å§‹æ–°ç»„
            if i in boundary_set or (temp_group and len(temp_group) >= args.chunk_size * 2):
                # ä¿å­˜å½“å‰ç»„
                if temp_group:
                    combined_tasks.append({
                        "indices": temp_indices,
                        "text": temp_text.strip(),
                        "similar_chunks": temp_group[0]["similar_chunks"],
                        "subtitles": [t["subtitle"] for t in temp_group]
                    })
                # å¼€å§‹æ–°ç»„
                temp_group = [task]
                temp_text = task["text"]
                temp_indices = [task["index"]]
            else:
                # æ·»åŠ åˆ°å½“å‰ç»„
                temp_group.append(task)
                temp_text += "\n\n" + task["text"]
                temp_indices.append(task["index"])
        
        # æ·»åŠ æœ€åä¸€ç»„
        if temp_group:
            combined_tasks.append({
                "indices": temp_indices,
                "text": temp_text.strip(),
                "similar_chunks": temp_group[0]["similar_chunks"],
                "subtitles": [t["subtitle"] for t in temp_group]
            })
        
        print(f"ğŸ“Š ä¼˜åŒ–ï¼šå°† {len(tasks)} æ¡å­—å¹•ç»„åˆä¸º {len(combined_tasks)} ä¸ªç¿»è¯‘ä»»åŠ¡")
        
        # å¢å¼ºç¿»è¯‘ä»»åŠ¡ï¼Œæ·»åŠ ç›¸ä¼¼åœºæ™¯ä¿¡æ¯
        enhanced_tasks = vector_tools.enhance_translation_consistency(
            combined_tasks,
            similar_scenes,
            subtitle_to_chunk_mapping
        )
        
        # æ‰¹å¤„ç†å‚æ•°
        batch_size = 50  # æ¯æ‰¹å¤„ç†çš„ä»»åŠ¡æ•°
        batch_delay = args.batch_delay  # æ‰¹æ¬¡é—´å»¶è¿Ÿ
        total = len(subtitles)  # æ€»å­—å¹•æ•°
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œç¿»è¯‘
        with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
            results = {}
            
            print(f"ğŸ§µ ä½¿ç”¨ {args.workers} ä¸ªå·¥ä½œçº¿ç¨‹è¿›è¡Œç¿»è¯‘ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}, å»¶è¿Ÿ: {batch_delay}ç§’ï¼‰...")
            print(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {ai_service.translation_model}")
            
            with tqdm(total=total, desc="ç¿»è¯‘è¿›åº¦") as pbar:
                # åˆ†æ‰¹æ¬¡å¤„ç†
                for i in range(0, len(enhanced_tasks), batch_size):
                    batch = enhanced_tasks[i:i+batch_size]
                    
                    # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                    futures = []
                    for task in batch:
                        # æå–ä¸Šä¸‹æ–‡å‚è€ƒï¼Œå¦‚æœæœ‰çš„è¯
                        context_refs = task.get("context_references", None)
                        
                        future = executor.submit(
                            ai_service.translate_subtitle_chunk,
                            task["text"],
                            args.lang,
                            content_analysis,
                            task["similar_chunks"],
                            context_refs
                        )
                        futures.append((future, task["indices"], task["subtitles"]))
                    
                    # æ”¶é›†ç»“æœ
                    for future, indices, batch_subtitles in futures:
                        try:
                            translation = future.result()
                            
                            if translation:
                                # æ‹†åˆ†ç¿»è¯‘ç»“æœå›å¤šä¸ªå­—å¹•
                                if len(indices) > 1:
                                    # ä¼˜åŒ–ï¼šä½¿ç”¨æ›´æ™ºèƒ½çš„æ‹†åˆ†æ–¹æ³•
                                    # é¦–å…ˆå°è¯•æŒ‰ç…§ç©ºè¡Œæ‹†åˆ†
                                    parts = translation.split("\n\n")
                                    
                                    # å¦‚æœæ‹†åˆ†å‡ºçš„éƒ¨åˆ†ä¸åŸå­—å¹•æ•°é‡åŒ¹é…
                                    if len(parts) == len(indices):
                                        for j, idx in enumerate(indices):
                                            results[idx] = parts[j]
                                    else:
                                        # å°è¯•æŒ‰å¥å­æ‹†åˆ†
                                        sentences = re.split(r'([ã€‚ï¼ï¼Ÿâ€¦])', translation)
                                        # åˆå¹¶å¥å­ä¸æ ‡ç‚¹
                                        real_sentences = []
                                        for k in range(0, len(sentences)-1, 2):
                                            if k+1 < len(sentences):
                                                real_sentences.append(sentences[k] + sentences[k+1])
                                            else:
                                                real_sentences.append(sentences[k])
                                        
                                        # å¦‚æœå¥å­æ•°é‡ä¸å­—å¹•æ•°é‡åŒ¹é…æˆ–æ¥è¿‘
                                        if abs(len(real_sentences) - len(indices)) <= 1:
                                            # å°½é‡å‡åŒ€åˆ†é…å¥å­åˆ°å­—å¹•
                                            sentences_per_subtitle = max(1, len(real_sentences) // len(indices))
                                            for j, idx in enumerate(indices):
                                                start = j * sentences_per_subtitle
                                                end = min(start + sentences_per_subtitle, len(real_sentences))
                                                if j == len(indices) - 1:  # æœ€åä¸€ä¸ªå­—å¹•æ‹¿å‰©ä¸‹æ‰€æœ‰å¥å­
                                                    end = len(real_sentences)
                                                if start < len(real_sentences):
                                                    results[idx] = "".join(real_sentences[start:end])
                                                else:
                                                    results[idx] = ""
                                        else:
                                            # æŒ‰æ¯”ä¾‹æ‹†åˆ†æ–‡æœ¬
                                            total_chars = len(translation)
                                            total_original_chars = sum([len(sub.text) for sub in batch_subtitles])
                                            
                                            start_pos = 0
                                            for j, idx in enumerate(indices):
                                                # æŒ‰åŸå§‹æ–‡æœ¬é•¿åº¦æ¯”ä¾‹åˆ†é…ç¿»è¯‘ç»“æœ
                                                original_ratio = len(batch_subtitles[j].text) / total_original_chars
                                                char_count = int(total_chars * original_ratio)
                                                
                                                # ç¡®ä¿ä¸ä¼šè¶Šç•Œ
                                                end_pos = min(start_pos + char_count, total_chars)
                                                
                                                # æå–å¯¹åº”çš„ç¿»è¯‘ç‰‡æ®µ
                                                results[idx] = translation[start_pos:end_pos].strip()
                                                start_pos = end_pos
                                else:
                                    # å•ä¸ªå­—å¹•ç›´æ¥ä½¿ç”¨æ•´ä¸ªç¿»è¯‘ç»“æœ
                                    results[indices[0]] = translation
                            else:
                                # å¦‚æœç¿»è¯‘ç»“æœä¸ºç©ºï¼Œä½¿ç”¨åŸæ–‡
                                for idx in indices:
                                    results[idx] = tasks[idx]["text"] if idx < len(tasks) else ""
                                    
                            # æ›´æ–°è¿›åº¦æ¡
                            pbar.update(len(indices))
                            pbar.set_description(f"ç¿»è¯‘è¿›åº¦ ({int(pbar.n/pbar.total*100)}%)")
                        except Exception as e:
                            print(f"âš ï¸ ç¿»è¯‘å¤±è´¥: {e}")
                            # å¤±è´¥æ—¶ä½¿ç”¨åŸæ–‡
                            for idx in indices:
                                if idx < len(tasks):
                                    results[idx] = tasks[idx]["text"]
                            pbar.update(len(indices))
                    
                    # æ¯æ‰¹æ¬¡ä¹‹é—´æ·»åŠ æçŸ­å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                    if i + batch_size < len(enhanced_tasks) and batch_delay > 0:
                        time.sleep(batch_delay)
            
            # æ›´æ–°å­—å¹•ç¿»è¯‘ç»“æœ
            for idx, translation in results.items():
                if idx < len(subtitles):
                    subtitles[idx].text = translation
        
        # å¦‚æœå¯ç”¨äº†å­—å¹•æ ¡å‡†åŠŸèƒ½
        if args.align:
            progress = 90
            print(f"ğŸ”„ æ­£åœ¨æ ¡å‡†å­—å¹•æ ¼å¼å’Œæ’ç‰ˆ... ({progress}%)")
            
            # å¯¼å…¥å­—å¹•æ ¡å‡†å·¥å…·
            from utils.subtitle_alignment import SubtitleAligner
            
            # åˆ›å»ºæºå­—å¹•è§£æå™¨çš„å‰¯æœ¬ä»¥ç”¨äºå¯¹æ¯”
            source_parser = SubtitleParser(args.input)
            
            # åˆ›å»ºå­—å¹•æ ¡å‡†å™¨
            subtitle_aligner = SubtitleAligner(ai_service)
            
            # æ‰§è¡Œå­—å¹•æ ¡å‡†
            aligned_parser = subtitle_aligner.align_subtitles(source_parser, subtitle_parser)
            
            # ä½¿ç”¨æ ¡å‡†åçš„å­—å¹•è§£æå™¨æ›¿æ¢åŸå­—å¹•è§£æå™¨
            subtitle_parser = aligned_parser
        
        # 5. ä¿å­˜ç¿»è¯‘ç»“æœ
        progress = 95
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ç¿»è¯‘ç»“æœåˆ° {output_file}... ({progress}%)")
        
        # ä¿å­˜SRTæ ¼å¼çš„å­—å¹•
        save_srt(subtitle_parser, output_file, encoding='utf-8')
        print(f"å­—å¹•å·²ä¿å­˜è‡³: {output_file}")
        
        # é¢å¤–ä¿å­˜åŸå§‹å­—å¹•
        original_srt_path = os.path.join(output_dir, base_name)
        shutil.copy2(args.input, original_srt_path)
        print(f"ğŸ“„ åŸå§‹å­—å¹•å·²å¤åˆ¶åˆ° {original_srt_path}")
        
        # ä¿å­˜å¤„ç†æ—¥å¿—
        log_file = os.path.join(output_dir, f"{movie_name}.log.json")
        log_data = {
            "input_file": args.input,
            "output_file": output_file,
            "target_language": args.lang,
            "subtitle_count": len(subtitles),
            "processed_at": datetime.now().isoformat(),
            "duration": metadata["duration"],
            "embedding_model": ai_service.embedding_model,
            "translation_model": ai_service.translation_model,
            "analysis_model": ai_service.analysis_model,
            "used_alignment": args.align
        }
        
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ å¤„ç†æ—¥å¿—å·²ä¿å­˜è‡³ {log_file}")
        
        # å®Œæˆ
        progress = 100
        print(f"âœ… ç¿»è¯‘å®Œæˆ! ({progress}%)")
        print(f"ğŸ“‚ æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²ä¿å­˜è‡³ {output_dir}")
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    exit(main()) 