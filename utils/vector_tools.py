import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Callable
import json
import os
from datetime import datetime
import pickle
import concurrent.futures
from tqdm import tqdm
import hashlib


class VectorTools:
    """å‘é‡å¤„ç†å’Œåˆ©ç”¨å·¥å…·"""
    
    def __init__(self, cache_dir: str = ".cache"):
        """
        åˆå§‹åŒ–å‘é‡å·¥å…·
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
        """
        self.cache_dir = cache_dir
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
    
    def compute_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: ç¬¬ä¸€ä¸ªå‘é‡
            vec2: ç¬¬äºŒä¸ªå‘é‡
            
        Returns:
            ç›¸ä¼¼åº¦å¾—åˆ†(0-1)
        """
        return VectorTools.static_compute_similarity(vec1, vec2)
    
    @staticmethod
    def static_compute_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¤šçº¿ç¨‹ç¯å¢ƒï¼‰
        
        Args:
            vec1: ç¬¬ä¸€ä¸ªå‘é‡
            vec2: ç¬¬äºŒä¸ªå‘é‡
            
        Returns:
            ç›¸ä¼¼åº¦å¾—åˆ†(0-1)
        """
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(v1, v2)
        norm_v1 = np.linalg.norm(v1)
        norm_v2 = np.linalg.norm(v2)
        
        # é˜²æ­¢é™¤é›¶é”™è¯¯
        if norm_v1 == 0 or norm_v2 == 0:
            return 0.0
            
        return dot_product / (norm_v1 * norm_v2)
    
    @staticmethod
    def _vectorize_chunk(chunk_with_index: Tuple[int, List[str]], embedding_func: Callable) -> Tuple[int, List[str], List[float]]:
        """å¤„ç†å¹¶å‘é‡åŒ–å•ä¸ªå—ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰
        
        Args:
            chunk_with_index: å…ƒç»„ (ç´¢å¼•, æ–‡æœ¬å—)
            embedding_func: å‘é‡åŒ–å‡½æ•°
            
        Returns:
            å…ƒç»„ (ç´¢å¼•, æ–‡æœ¬å—, å‘é‡)
        """
        idx, chunk = chunk_with_index
        text = " ".join(chunk)
        embedding = embedding_func(text)
        return (idx, chunk, embedding)
    
    def chunk_and_vectorize(self, texts: List[str], embedding_func: Callable, chunk_size: int = 3, max_workers: int = 3) -> Dict[str, Any]:
        """å°†æ–‡æœ¬åˆ†å—å¹¶å‘é‡åŒ–
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            embedding_func: å‘é‡åŒ–å‡½æ•°
            chunk_size: æ¯å—çš„æ–‡æœ¬æ•°é‡
            max_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            
        Returns:
            Dict: åŒ…å«å—å’Œå¯¹åº”çš„å‘é‡
        """
        # ä¼˜åŒ–ï¼šè·³è¿‡ç©ºæ–‡æœ¬ï¼Œå‡å°‘å¤„ç†é‡
        non_empty_texts = [t for t in texts if t and t.strip()]
        
        # åˆ†å— - ä¼˜åŒ–åˆ†å—ç­–ç•¥ï¼Œç¡®ä¿è¯­ä¹‰å®Œæ•´æ€§
        chunks = []
        for i in range(0, len(non_empty_texts), chunk_size):
            # æå–å½“å‰å—ä¸­çš„æ–‡æœ¬
            current_chunk = non_empty_texts[i:i+chunk_size]
            chunks.append(current_chunk)
        
        # å¤šçº¿ç¨‹å‘é‡åŒ–ï¼Œä½¿ç”¨çº¿ç¨‹æ± ä¼˜åŒ–
        chunk_embeddings = [None] * len(chunks)
        
        # åˆ›å»ºå¸¦ç´¢å¼•çš„ä»»åŠ¡åˆ—è¡¨
        tasks = [(i, chunk) for i, chunk in enumerate(chunks)]
        
        # æ‰¹å¤„ç†ä¼˜åŒ–ï¼šæ¯æ¬¡å¤„ç†å¤šä¸ªå—ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
        batch_size = min(10, len(chunks))  # æ¯æ‰¹æœ€å¤š10ä¸ªå—
        
        # åˆ›å»ºè¿›åº¦æ¡
        with tqdm(total=len(chunks), desc="å‘é‡åŒ–è¿›åº¦") as pbar:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # åˆ†æ‰¹æäº¤ä»»åŠ¡
                for i in range(0, len(tasks), batch_size):
                    batch_tasks = tasks[i:i+batch_size]
                    
                    # æäº¤å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                    futures = []
                    for task in batch_tasks:
                        future = executor.submit(self._vectorize_chunk, task, embedding_func)
                        futures.append(future)
                    
                    # å¤„ç†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
                    for future in concurrent.futures.as_completed(futures):
                        idx, chunk, embedding = future.result()
                        chunk_embeddings[idx] = embedding
                        chunks[idx] = chunk
                        # æ›´æ–°è¿›åº¦æ¡
                        pbar.update(1)
                        pbar.set_description(f"å‘é‡åŒ–è¿›åº¦ ({int(pbar.n/pbar.total*100)}%)")
        
        return {
            "chunks": chunks,
            "embeddings": chunk_embeddings,
            "total_chunks": len(chunks)
        }
    
    def find_similar_chunks(self, query_embedding: List[float], chunk_embeddings: List[List[float]], k: int = 2) -> List[int]:
        """æŸ¥æ‰¾ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„kä¸ªå—
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            chunk_embeddings: å—å‘é‡åˆ—è¡¨
            k: è¿”å›çš„ç›¸ä¼¼å—æ•°é‡
            
        Returns:
            List[int]: ç›¸ä¼¼å—çš„ç´¢å¼•
        """
        return VectorTools.static_find_similar_chunks(query_embedding, chunk_embeddings, k)
    
    @staticmethod
    def static_find_similar_chunks(query_embedding: List[float], chunk_embeddings: List[List[float]], top_k: int = 2) -> List[int]:
        """é™æ€æ–¹æ³•ï¼šæŸ¥æ‰¾ä¸æŸ¥è¯¢å‘é‡æœ€ç›¸ä¼¼çš„kä¸ªå—ï¼ˆç”¨äºå¤šçº¿ç¨‹ç¯å¢ƒï¼‰
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            chunk_embeddings: å—å‘é‡åˆ—è¡¨
            top_k: è¿”å›çš„ç›¸ä¼¼å—æ•°é‡
            
        Returns:
            List[int]: ç›¸ä¼¼å—çš„ç´¢å¼•
        """
        if not chunk_embeddings or not query_embedding:
            return []
            
        similarities = []
        for i, embedding in enumerate(chunk_embeddings):
            if embedding:  # ç¡®ä¿åµŒå…¥å­˜åœ¨
                similarity = VectorTools.static_compute_similarity(query_embedding, embedding)
                similarities.append((i, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # è¿”å›å‰kä¸ªç›¸ä¼¼å—çš„ç´¢å¼•
        return [idx for idx, _ in similarities[:top_k]]
    
    def extract_key_terms(self, 
                         full_embedding: List[float], 
                         term_embeddings: Dict[str, List[float]], 
                         threshold: float = 0.8) -> List[str]:
        """
        æ ¹æ®å‘é‡ç›¸ä¼¼åº¦æå–å…³é”®æœ¯è¯­
        
        Args:
            full_embedding: å®Œæ•´å­—å¹•çš„å‘é‡è¡¨ç¤º
            term_embeddings: æœ¯è¯­åŠå…¶å‘é‡çš„å­—å…¸
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸å…³åº¦é«˜çš„æœ¯è¯­åˆ—è¡¨
        """
        return VectorTools.static_extract_key_terms(full_embedding, term_embeddings, threshold)
    
    @staticmethod
    def static_extract_key_terms(full_embedding: List[float], 
                               term_embeddings: Dict[str, List[float]], 
                               threshold: float = 0.8) -> List[str]:
        """
        æ ¹æ®å‘é‡ç›¸ä¼¼åº¦æå–å…³é”®æœ¯è¯­ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå¤šçº¿ç¨‹ç¯å¢ƒï¼‰
        
        Args:
            full_embedding: å®Œæ•´å­—å¹•çš„å‘é‡è¡¨ç¤º
            term_embeddings: æœ¯è¯­åŠå…¶å‘é‡çš„å­—å…¸
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            ç›¸å…³åº¦é«˜çš„æœ¯è¯­åˆ—è¡¨
        """
        relevant_terms = []
        
        for term, term_emb in term_embeddings.items():
            similarity = VectorTools.static_compute_similarity(full_embedding, term_emb)
            if similarity >= threshold:
                relevant_terms.append(term)
        
        return relevant_terms
    
    def get_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: MD5å“ˆå¸Œå€¼
        """
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            buf = f.read()
            hasher.update(buf)
        return hasher.hexdigest()
    
    def cache_embeddings(self, file_path: str, embeddings_data: Dict[str, Any]) -> None:
        """ç¼“å­˜åµŒå…¥å‘é‡æ•°æ®
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            embeddings_data: åµŒå…¥å‘é‡æ•°æ®
        """
        file_hash = self.get_file_hash(file_path)
        cache_path = os.path.join(self.cache_dir, f"{file_hash}.json")
        
        # å‡†å¤‡ç¼“å­˜æ•°æ®
        cache_data = {
            "file_path": file_path,
            "file_hash": file_hash,
            "chunks": embeddings_data["chunks"],
            "embeddings": embeddings_data["embeddings"],
            "total_chunks": len(embeddings_data["chunks"])
        }
        
        if "full_embedding" in embeddings_data:
            cache_data["full_embedding"] = embeddings_data["full_embedding"]
            
        if "vector_chunk_size" in embeddings_data:
            cache_data["vector_chunk_size"] = embeddings_data["vector_chunk_size"]
        
        # ä¿å­˜ç¼“å­˜
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False)
    
    def load_cached_embeddings(self, file_path: str) -> Dict[str, Any]:
        """åŠ è½½ç¼“å­˜çš„åµŒå…¥å‘é‡æ•°æ®
        
        Args:
            file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: ç¼“å­˜çš„åµŒå…¥å‘é‡æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰ç¼“å­˜åˆ™è¿”å›None
        """
        file_hash = self.get_file_hash(file_path)
        cache_path = os.path.join(self.cache_dir, f"{file_hash}.json")
        
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                # éªŒè¯ç¼“å­˜
                if cache_data.get("file_hash") == file_hash:
                    return cache_data
            except Exception as e:
                print(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def get_context_enhanced_subtitle(self, 
                                    subtitle_text: str, 
                                    similar_chunks: List[str], 
                                    key_terms: List[str]) -> str:
        """
        ç”Ÿæˆå¢å¼ºä¸Šä¸‹æ–‡çš„å­—å¹•æ–‡æœ¬
        
        Args:
            subtitle_text: åŸå­—å¹•æ–‡æœ¬
            similar_chunks: ç›¸ä¼¼å—å†…å®¹
            key_terms: å…³é”®æœ¯è¯­
            
        Returns:
            å¢å¼ºä¸Šä¸‹æ–‡çš„å­—å¹•æ–‡æœ¬
        """
        context = ""
        
        # æ·»åŠ ç›¸ä¼¼å—å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
        if similar_chunks:
            context += "ç›¸å…³ä¸Šä¸‹æ–‡:\n"
            for i, chunk in enumerate(similar_chunks[:3]):  # æœ€å¤šä½¿ç”¨3ä¸ªç›¸ä¼¼å—
                context += f"ä¸Šä¸‹æ–‡ {i+1}: {chunk}\n"
        
        # æ·»åŠ å…³é”®æœ¯è¯­
        if key_terms:
            context += "\nå…³é”®æœ¯è¯­:\n"
            context += ", ".join(key_terms)
        
        # å°†åŸæ–‡å’Œä¸Šä¸‹æ–‡ç»„åˆ
        enhanced_text = f"{subtitle_text}\n\n{context}"
        
        return enhanced_text
    
    def find_scene_boundaries(self, embeddings: List[List[float]], threshold: float = 0.7) -> List[int]:
        """
        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ£€æµ‹å¯èƒ½çš„åœºæ™¯è¾¹ç•Œ
        
        Args:
            embeddings: åµŒå…¥å‘é‡åˆ—è¡¨
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¢«è§†ä¸ºåœºæ™¯è¾¹ç•Œ
            
        Returns:
            å¯èƒ½çš„åœºæ™¯è½¬æ¢ç‚¹ç´¢å¼•åˆ—è¡¨
        """
        if not embeddings or len(embeddings) < 2:
            return []
            
        boundaries = []
        
        # è®¡ç®—ç›¸é‚»å‘é‡ä¹‹é—´çš„ç›¸ä¼¼åº¦
        for i in range(1, len(embeddings)):
            prev_embedding = embeddings[i-1]
            curr_embedding = embeddings[i]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = self.compute_similarity(prev_embedding, curr_embedding)
            
            # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œå¯èƒ½æ˜¯åœºæ™¯è½¬æ¢
            if similarity < threshold:
                boundaries.append(i)
                
        return boundaries
        
    def find_similar_scenes(self, 
                            chunk_embeddings: List[List[float]], 
                            scene_boundaries: List[int], 
                            similarity_threshold: float = 0.75) -> Dict[int, List[int]]:
        """
        æ‰¾å‡ºå­—å¹•ä¸­ç›¸ä¼¼çš„åœºæ™¯ï¼Œç”¨äºæé«˜ç¿»è¯‘ä¸€è‡´æ€§
        
        Args:
            chunk_embeddings: å—çš„åµŒå…¥å‘é‡åˆ—è¡¨
            scene_boundaries: åœºæ™¯è¾¹ç•Œç´¢å¼•åˆ—è¡¨
            similarity_threshold: åœºæ™¯ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            åœºæ™¯æ˜ å°„å­—å…¸ï¼Œé”®ä¸ºåœºæ™¯ç´¢å¼•ï¼Œå€¼ä¸ºç›¸ä¼¼åœºæ™¯ç´¢å¼•åˆ—è¡¨
        """
        print(f"ğŸ” è¯†åˆ«ç›¸ä¼¼åœºæ™¯ï¼Œä»¥æé«˜ç¿»è¯‘ä¸€è‡´æ€§...")
        
        # å¤„ç†æœ‰æ•ˆè¾¹ç•Œ
        boundaries = sorted(list(set([0] + scene_boundaries + [len(chunk_embeddings)])))
        
        # æå–æ¯ä¸ªåœºæ™¯çš„å¹³å‡å‘é‡
        scene_vectors = []
        scene_ranges = []
        
        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i+1]
            
            # æå–å½“å‰åœºæ™¯çš„æ‰€æœ‰å‘é‡
            scene_chunks = chunk_embeddings[start:end]
            if not scene_chunks:
                continue
                
            # è®¡ç®—å¹³å‡å‘é‡ä»£è¡¨æ•´ä¸ªåœºæ™¯
            scene_vector = np.mean(scene_chunks, axis=0).tolist()
            scene_vectors.append(scene_vector)
            scene_ranges.append((start, end))
        
        # è®¡ç®—åœºæ™¯é—´ç›¸ä¼¼åº¦å¹¶æ‰¾å‡ºç›¸ä¼¼åœºæ™¯
        similar_scenes = {}
        for i, scene_i in enumerate(scene_vectors):
            similar_to_i = []
            
            for j, scene_j in enumerate(scene_vectors):
                if i == j:
                    continue
                    
                similarity = self.compute_similarity(scene_i, scene_j)
                if similarity > similarity_threshold:
                    similar_to_i.append(j)
            
            if similar_to_i:
                similar_scenes[i] = similar_to_i
        
        # è½¬æ¢ä¸ºå—ç´¢å¼•çš„æ˜ å°„
        chunk_similarities = {}
        for scene_idx, similar_scenes_idx in similar_scenes.items():
            start, end = scene_ranges[scene_idx]
            
            # ä¸ºå½“å‰åœºæ™¯ä¸­çš„æ¯ä¸ªå—å»ºç«‹ç›¸ä¼¼æ˜ å°„
            for chunk_idx in range(start, end):
                similar_chunks = []
                
                # æ·»åŠ æ‰€æœ‰ç›¸ä¼¼åœºæ™¯ä¸­çš„å—
                for sim_scene_idx in similar_scenes_idx:
                    sim_start, sim_end = scene_ranges[sim_scene_idx]
                    similar_chunks.extend(range(sim_start, sim_end))
                
                chunk_similarities[chunk_idx] = similar_chunks
        
        if chunk_similarities:
            print(f"âœ… æ‰¾åˆ° {len(chunk_similarities)} ä¸ªå—å…·æœ‰ç›¸ä¼¼åœºæ™¯")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ˜æ˜¾çš„ç›¸ä¼¼åœºæ™¯")
            
        return chunk_similarities
    
    def enhance_translation_consistency(self, 
                                       translation_tasks: List[Dict], 
                                       similar_scenes: Dict[int, List[int]],
                                       chunk_mapping: Dict[int, int]) -> List[Dict]:
        """
        å¢å¼ºç¿»è¯‘ä¸€è‡´æ€§ï¼Œä¸ºç›¸ä¼¼åœºæ™¯æ·»åŠ ä¸Šä¸‹æ–‡å‚è€ƒ
        
        Args:
            translation_tasks: ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
            similar_scenes: ç›¸ä¼¼åœºæ™¯æ˜ å°„
            chunk_mapping: å­—å¹•ç´¢å¼•åˆ°å—ç´¢å¼•çš„æ˜ å°„
            
        Returns:
            å¢å¼ºåçš„ç¿»è¯‘ä»»åŠ¡åˆ—è¡¨
        """
        print("ğŸ”„ å¢å¼ºç¿»è¯‘ä¸€è‡´æ€§...")
        enhanced_tasks = []
        
        # æ•´ç†å­—å¹•ç´¢å¼•åˆ°ç›¸ä¼¼åœºæ™¯çš„æ˜ å°„
        subtitle_to_similar_chunks = {}
        for subtitle_idx, chunk_idx in chunk_mapping.items():
            if chunk_idx in similar_scenes:
                subtitle_to_similar_chunks[subtitle_idx] = similar_scenes[chunk_idx]
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡æ·»åŠ ç›¸ä¼¼åœºæ™¯ä¿¡æ¯
        for task in translation_tasks:
            task_copy = task.copy()
            
            # æ£€æŸ¥ä»»åŠ¡ä¸­çš„å­—å¹•æ˜¯å¦æœ‰ç›¸ä¼¼åœºæ™¯
            for idx in task["indices"]:
                if idx in subtitle_to_similar_chunks:
                    # æ‰¾å‡ºä»»åŠ¡ä¸­éœ€è¦æ·»åŠ åˆ°ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼å—
                    similar_chunk_indices = subtitle_to_similar_chunks[idx]
                    context_references = []
                    
                    # å°†ç›¸ä¼¼å—çš„ä¿¡æ¯æ·»åŠ åˆ°ç¿»è¯‘ä¸Šä¸‹æ–‡ä¸­
                    for chunk_idx in similar_chunk_indices[:3]:  # é™åˆ¶æœ€å¤š3ä¸ªå‚è€ƒå—
                        for ref_task in translation_tasks:
                            if any(ref_idx in chunk_mapping and chunk_mapping[ref_idx] == chunk_idx for ref_idx in ref_task["indices"]):
                                context_references.append({
                                    "text": ref_task["text"],
                                    "relevance": "high",
                                    "reason": "ç›¸ä¼¼åœºæ™¯"
                                })
                    
                    # å¦‚æœæ‰¾åˆ°ç›¸ä¼¼å†…å®¹ï¼Œæ·»åŠ åˆ°ä»»åŠ¡ä¸­
                    if context_references:
                        if "context_references" not in task_copy:
                            task_copy["context_references"] = []
                        task_copy["context_references"].extend(context_references)
                        break  # ä¸€ä¸ªä»»åŠ¡åªéœ€è¦æ·»åŠ ä¸€æ¬¡ç›¸ä¼¼åœºæ™¯
            
            enhanced_tasks.append(task_copy)
        
        print(f"âœ… å·²ä¸º {sum(1 for t in enhanced_tasks if 'context_references' in t)} ä¸ªç¿»è¯‘ä»»åŠ¡å¢å¼ºä¸Šä¸‹æ–‡ä¸€è‡´æ€§")
        return enhanced_tasks 