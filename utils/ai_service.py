import os
import time
from typing import List, Dict, Any, Optional
from openai import OpenAI
from dotenv import load_dotenv
import numpy as np
import json
import re

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class AIService:
    """AIæœåŠ¡æ¥å£"""
    
    def __init__(self, embedding_model=None, translation_model=None, analysis_model=None):
        """
        åˆå§‹åŒ–AIæœåŠ¡
        
        Args:
            embedding_model: åµŒå…¥æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
            translation_model: ç¿»è¯‘æ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
            analysis_model: åˆ†ææ¨¡å‹åç§°ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
        """
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.api_base = os.getenv("OPENAI_API_BASE")
        
        # ä¼˜å…ˆä½¿ç”¨å‚æ•°æŒ‡å®šçš„æ¨¡å‹ï¼Œå¦åˆ™ä½¿ç”¨ç¯å¢ƒå˜é‡
        self.embedding_model = embedding_model or os.getenv("EMBEDDING_MODEL", "text-embedding-ada-002")
        self.translation_model = translation_model or os.getenv("TRANSLATION_MODEL", "gpt-3.5-turbo")
        self.analysis_model = analysis_model or os.getenv("ANALYSIS_MODEL", "gpt-3.5-turbo")
        
        if not self.api_key:
            raise ValueError("æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # å¢å¼ºAPIæ€§èƒ½ä¼˜åŒ–
        if self.api_base:
            self.client = OpenAI(
                api_key=self.api_key, 
                base_url=self.api_base,
                timeout=120,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿï¼Œé˜²æ­¢å¤§æ‰¹é‡è¯·æ±‚è¶…æ—¶
                max_retries=10,  # å¢åŠ é‡è¯•æ¬¡æ•°åˆ°10æ¬¡ï¼Œæé«˜å¯é æ€§
                default_headers={"User-Agent": "SubtitleTranslator/1.0"}  # è‡ªå®šä¹‰å¤´éƒ¨ä¾¿äºAPIè¯†åˆ«
            )
        else:
            self.client = OpenAI(
                api_key=self.api_key,
                timeout=120,
                max_retries=10,
                default_headers={"User-Agent": "SubtitleTranslator/1.0"}
            )
        
        # ç”¨äºç¼“å­˜åˆ†æå‡ºçš„æœ¯è¯­å‘é‡
        self.term_embeddings = {}
        
        # ç”¨äºç¼“å­˜ç¿»è¯‘ç»“æœï¼Œé¿å…é‡å¤ç¿»è¯‘
        self.translation_cache = {}
        
        # è®°å½•ä½¿ç”¨çš„æ¨¡å‹
        print(f"ğŸ¤– ä½¿ç”¨åµŒå…¥æ¨¡å‹: {self.embedding_model}")
        print(f"ğŸ¤– ä½¿ç”¨ç¿»è¯‘æ¨¡å‹: {self.translation_model}")
        print(f"ğŸ¤– ä½¿ç”¨åˆ†ææ¨¡å‹: {self.analysis_model}")
    
    def get_embedding(self, text: str) -> List[float]:
        """
        è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            åµŒå…¥å‘é‡
        """
        max_retries = 5
        retry_delay = 2
        
        # å¦‚æœæ–‡æœ¬è¶…é•¿ï¼Œåˆ†æ®µå¤„ç†å¹¶åˆå¹¶å‘é‡
        max_chars = 4000  # çº¦5000-6000 tokensï¼Œè¶³å¤Ÿå®‰å…¨
        if len(text) > max_chars:
            print(f"âš ï¸ æ–‡æœ¬è¿‡é•¿ ({len(text)}å­—ç¬¦)ï¼Œå°†åˆ†æ®µå¤„ç†å¹¶åˆå¹¶å‘é‡")
            # åˆ†æ®µå¤„ç†æ–‡æœ¬
            segments = []
            for i in range(0, len(text), max_chars):
                segments.append(text[i:i+max_chars])
            
            # è·å–æ¯æ®µæ–‡æœ¬çš„å‘é‡
            segment_embeddings = []
            for i, segment in enumerate(segments):
                print(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(segments)} æ®µæ–‡æœ¬...")
                for attempt in range(max_retries):
                    try:
                        response = self.client.embeddings.create(
                            model=self.embedding_model,
                            input=segment
                        )
                        segment_embeddings.append(response.data[0].embedding)
                        break
                    except Exception as e:
                        if attempt < max_retries - 1:
                            print(f"è·å–åµŒå…¥å‘é‡å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt+1}/{max_retries}): {e}")
                            time.sleep(retry_delay)
                        else:
                            raise ValueError(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
            
            # åˆå¹¶å‘é‡ï¼ˆç®€å•å¹³å‡ï¼‰
            if segment_embeddings:
                combined_embedding = np.mean(segment_embeddings, axis=0)
                return combined_embedding.tolist()
            else:
                raise ValueError("æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•æ®µè½çš„åµŒå…¥å‘é‡")
        
        # å¯¹äºæ­£å¸¸é•¿åº¦çš„æ–‡æœ¬ï¼Œç›´æ¥å¤„ç†
        for attempt in range(max_retries):
            try:
                response = self.client.embeddings.create(
                    model=self.embedding_model,
                    input=text
                )
                return response.data[0].embedding
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"è·å–åµŒå…¥å‘é‡å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt+1}/{max_retries}): {e}")
                    time.sleep(retry_delay)
                else:
                    raise ValueError(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
    
    def get_batch_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡
        
        Args:
            texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°
        retry_delay = 1  # å‡å°‘é‡è¯•å»¶è¿Ÿ
        max_chars = 4000  # æ¯æ®µæœ€å¤§å­—ç¬¦æ•°
        
        # ç¼“å­˜ç»“æœé¿å…é‡å¤è®¡ç®—
        cached_results = []
        texts_to_embed = []
        original_indices = []
        
        # æ£€æŸ¥å“ªäº›æ–‡æœ¬éœ€è¦è®¡ç®—å‘é‡
        for i, text in enumerate(texts):
            # ä½¿ç”¨MD5ä½œä¸ºç¼“å­˜é”®
            cache_key = f"emb_{hash(text) % 10000000}"
            if cache_key in self.term_embeddings:
                cached_results.append((i, self.term_embeddings[cache_key]))
            else:
                texts_to_embed.append(text)
                original_indices.append(i)
        
        if not texts_to_embed:
            # æ‰€æœ‰å‘é‡å·²ç¼“å­˜
            results = [None] * len(texts)
            for i, emb in cached_results:
                results[i] = emb
            return results
            
        # æ‰¹é‡è°ƒç”¨APIè·å–å‘é‡
        # å°†é•¿æ–‡æœ¬åˆ—è¡¨åˆ†æˆæ›´å°çš„æ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤š20ä¸ªæ–‡æœ¬
        all_embeddings = []
        for i in range(0, len(texts_to_embed), 20):
            batch = texts_to_embed[i:i+20]
            processed_batch = []
            
            # å¤„ç†æ¯ä¸ªæ–‡æœ¬ï¼Œç¡®ä¿ä¸è¶…è¿‡é•¿åº¦é™åˆ¶
            for text in batch:
                if len(text) > max_chars:
                    # åˆ†æ®µå¤„ç†è¶…é•¿æ–‡æœ¬
                    segments = []
                    for j in range(0, len(text), max_chars):
                        segments.append(text[j:j+max_chars])
                    
                    # è·å–æ¯æ®µçš„å‘é‡
                    segment_embeddings = []
                    for segment in segments:
                        for attempt in range(max_retries):
                            try:
                                response = self.client.embeddings.create(
                                    model=self.embedding_model,
                                    input=segment
                                )
                                segment_embeddings.append(response.data[0].embedding)
                                break
                            except Exception as e:
                                if attempt < max_retries - 1:
                                    print(f"è·å–åµŒå…¥å‘é‡å¤±è´¥ï¼Œæ­£åœ¨é‡è¯•: {e}")
                                    time.sleep(retry_delay)
                                else:
                                    raise ValueError(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
                    
                    # åˆå¹¶æ®µè½å‘é‡
                    if segment_embeddings:
                        combined_embedding = np.mean(segment_embeddings, axis=0)
                        all_embeddings.append(combined_embedding.tolist())
                    else:
                        # å¦‚æœæ²¡æœ‰è·å–åˆ°ä»»ä½•å‘é‡ï¼Œä½¿ç”¨é›¶å‘é‡
                        raise ValueError("æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•æ®µè½çš„åµŒå…¥å‘é‡")
                else:
                    processed_batch.append(text)
            
            # å¯¹æ­£å¸¸é•¿åº¦çš„æ–‡æœ¬æ‰¹é‡å¤„ç†
            if processed_batch:
                for attempt in range(max_retries):
                    try:
                        response = self.client.embeddings.create(
                            model=self.embedding_model,
                            input=processed_batch
                        )
                        all_embeddings.extend([item.embedding for item in response.data])
                        break
                    except Exception as e:
                        if attempt < max_retries - 1:
                            print(f"è·å–æ‰¹é‡åµŒå…¥å‘é‡å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt+1}/{max_retries}): {e}")
                            time.sleep(retry_delay)
                        else:
                            raise ValueError(f"è·å–æ‰¹é‡åµŒå…¥å‘é‡å¤±è´¥: {e}")
        
        # åˆå¹¶ç»“æœ
        results = [None] * len(texts)
        
        # æ·»åŠ å·²ç¼“å­˜çš„ç»“æœ
        for i, emb in cached_results:
            results[i] = emb
        
        # æ·»åŠ æ–°è®¡ç®—çš„ç»“æœå¹¶ç¼“å­˜
        for i, idx in enumerate(original_indices):
            if i < len(all_embeddings):  # é˜²æ­¢ç´¢å¼•è¶Šç•Œ
                results[idx] = all_embeddings[i]
                
                # ç¼“å­˜ç»“æœ
                cache_key = f"emb_{hash(texts[idx]) % 10000000}"
                self.term_embeddings[cache_key] = all_embeddings[i]
        
        return results
    
    def analyze_content(self, text, max_length=15000):
        """åˆ†æå­—å¹•å†…å®¹ï¼Œè¯†åˆ«ç±»å‹ã€ä¸»é¢˜å’Œå…³é”®æœ¯è¯­
        
        Args:
            text: å­—å¹•æ–‡æœ¬
            max_length: æœ€å¤§å¤„ç†æ–‡æœ¬é•¿åº¦
            
        Returns:
            Dict: å†…å®¹åˆ†æç»“æœ
        """
        # é™åˆ¶æ–‡æœ¬é•¿åº¦
        text = text[:max_length]
        
        try:
            prompt = """åˆ†æä»¥ä¸‹å­—å¹•æ–‡æœ¬ï¼Œå¹¶æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. å½±ç‰‡ç±»å‹/é£æ ¼ï¼ˆå¦‚åŠ¨ä½œã€å–œå‰§ã€ææ€–ã€ç§‘å¹»ç­‰ï¼‰
2. å‰§æƒ…ç®€è¦æ¦‚è¿°
3. å…³é”®äººç‰©åŠå…¶ç‰¹ç‚¹
4. å‡ºç°çš„ä¸“ä¸šæœ¯è¯­æˆ–ç‰¹æ®Šè¯æ±‡
5. æƒ…æ„ŸåŸºè°ƒ

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œä½¿ç”¨ä»¥ä¸‹ç»“æ„ï¼š
{
  "genre": "ç”µå½±ç±»å‹",
  "plot_summary": "å‰§æƒ…æ¦‚è¿°",
  "characters": ["è§’è‰²1", "è§’è‰²2"],
  "terminology": ["æœ¯è¯­1", "æœ¯è¯­2"],
  "tone": "æƒ…æ„ŸåŸºè°ƒ"
}

åªè¿”å›JSONç»“æ„ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Šã€‚"""
            
            response = self.client.chat.completions.create(
                model=self.analysis_model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†å†…å®¹åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»å­—å¹•ä¸­åˆ†æå½±ç‰‡å†…å®¹ã€‚"},
                    {"role": "user", "content": f"{prompt}\n\nå­—å¹•æ–‡æœ¬:\n{text}"}
                ],
                response_format={"type": "json_object"}
            )
            
            result = response.choices[0].message.content
            
            # è§£æJSONç»“æœ
            try:
                analysis = json.loads(result)
                return analysis
            except json.JSONDecodeError as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                # è¿”å›åŸºæœ¬ç»“æ„
                return {
                    "genre": "æœªçŸ¥",
                    "plot_summary": "æ— æ³•è§£æå‰§æƒ…",
                    "characters": [],
                    "terminology": [],
                    "tone": "æœªçŸ¥"
                }
                
        except Exception as e:
            print(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            raise
    
    def build_terminology_embeddings(self, content_analysis: Dict[str, Any]) -> Dict[str, List[float]]:
        """
        ä¸ºåˆ†æå‡ºçš„æœ¯è¯­ç”ŸæˆåµŒå…¥å‘é‡
        
        Args:
            content_analysis: å†…å®¹åˆ†æç»“æœ
            
        Returns:
            æœ¯è¯­åŠå…¶å‘é‡çš„å­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æœ¯è¯­ï¼Œè¿”å›ç©ºå­—å…¸
        if not content_analysis or "terminology" not in content_analysis:
            return {}
        
        terminology = content_analysis["terminology"]
        terms = list(terminology.keys())
        
        if not terms:
            return {}
        
        try:
            # æ‰¹é‡è·å–æœ¯è¯­çš„åµŒå…¥å‘é‡
            term_embeddings = {}
            batch_embeddings = self.get_batch_embeddings(terms)
            
            for i, term in enumerate(terms):
                term_embeddings[term] = batch_embeddings[i]
            
            # ç¼“å­˜ç»“æœ
            self.term_embeddings = term_embeddings
            return term_embeddings
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆæœ¯è¯­å‘é‡å¤±è´¥: {e}")
            return {}
    
    def translate_subtitle(self, 
                          subtitle_text: str,
                          target_language: str = "zh-CN",
                          content_analysis: Optional[Dict[str, Any]] = None) -> str:
        """
        ç¿»è¯‘å­—å¹•æ–‡æœ¬
        
        Args:
            subtitle_text: å­—å¹•æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€
            content_analysis: å†…å®¹åˆ†æç»“æœ
            
        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        context = ""
        if content_analysis:
            context = f"""
            ç”µå½±ç±»å‹ä¸ä¸»é¢˜: {content_analysis.get('genre', '')}
            ä¸»è¦å†…å®¹: {content_analysis.get('content', '')}
            ä¸“ä¸šæœ¯è¯­: {content_analysis.get('terminology', {})}
            äººç‰©å…³ç³»: {content_analysis.get('characters', {})}
            ç‰¹æ®Šè¯­å¢ƒ: {content_analysis.get('context', '')}
            """
        
        system_prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å­—å¹•ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å°†ä»¥ä¸‹å­—å¹•ç¿»è¯‘æˆ{target_language}ã€‚
        
        ç¿»è¯‘æŒ‡å—:
        1. ä¿æŒåŸæ„çš„åŒæ—¶ï¼Œä½¿è¯‘æ–‡è‡ªç„¶æµç•…ï¼Œç¬¦åˆç›®æ ‡è¯­è¨€ä¹ æƒ¯
        2. æ ¹æ®å½±ç‰‡ç±»å‹å’Œåœºæ™¯è°ƒæ•´ç”¨è¯å’Œè¯­æ°”
        3. ä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§
        4. æ³¨æ„äººç‰©å¯¹è¯çš„è¯­æ°”å’Œä¸ªæ€§
        5. è€ƒè™‘æ–‡åŒ–å·®å¼‚ï¼Œé€‚å½“æœ¬åœ°åŒ–
        
        ä»¥ä¸‹æ˜¯å…³äºå½±ç‰‡çš„èƒŒæ™¯ä¿¡æ¯ï¼Œè¯·åœ¨ç¿»è¯‘æ—¶å‚è€ƒ:
        {context}
        
        è¯·åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚
        """
        
        try:
            response = self.client.chat.completions.create(
                model=self.translation_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": subtitle_text}
                ]
            )
            return response.choices[0].message.content
        except Exception as e:
            raise ValueError(f"ç¿»è¯‘å¤±è´¥: {e}")
    
    def translate_with_context(self, subtitle_text, target_language="zh-CN", content_analysis=None, similar_chunks=None, relevant_terms=None):
        """ä½¿ç”¨ä¸Šä¸‹æ–‡ç¿»è¯‘å­—å¹•
        
        Args:
            subtitle_text: å­—å¹•æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€
            content_analysis: å†…å®¹åˆ†æç»“æœ
            similar_chunks: ç›¸ä¼¼æ–‡æœ¬å—
            relevant_terms: ç›¸å…³æœ¯è¯­
            
        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        # å¦‚æœå­—å¹•æ–‡æœ¬ä¸ºç©ºï¼Œç›´æ¥è¿”å›
        if not subtitle_text or subtitle_text.strip() == "":
            return subtitle_text

        # æ„å»ºä¸Šä¸‹æ–‡
        context = ""
        
        # æ·»åŠ å†…å®¹åˆ†æä¸Šä¸‹æ–‡ï¼ˆä¸°å¯Œå¤„ç†ä»¥æé«˜ä¼ é€’ç»™æ¨¡å‹çš„ä¿¡æ¯é‡ï¼‰
        if content_analysis:
            # ä¸ºç¿»è¯‘æä¾›æ›´è¯¦ç»†çš„å†…å®¹èƒŒæ™¯
            if 'genre' in content_analysis:
                context += f"ç±»å‹: {content_analysis['genre']}\n"
            
            if 'plot_summary' in content_analysis:
                context += f"å‰§æƒ…æ¦‚è¦: {content_analysis['plot_summary']}\n"
            
            if 'main_characters' in content_analysis:
                # æä¾›ä¸»è¦è§’è‰²ä¿¡æ¯ä»¥ä¿æŒäººç‰©ç§°å‘¼ä¸€è‡´æ€§
                characters = content_analysis['main_characters']
                if characters and len(characters) > 0:
                    character_text = ", ".join([f"{name}" for name in characters])
                    context += f"ä¸»è¦è§’è‰²: {character_text}\n"
            
            # ç¡®ä¿æœ¯è¯­ç¿»è¯‘ä¸€è‡´
            if 'terminology' in content_analysis:
                terms = content_analysis['terminology']
                if terms and len(terms) > 0:
                    term_text = ", ".join([f"{term}" for term in terms])
                    context += f"æœ¯è¯­: {term_text}\n"
            
            # æä¾›æƒ…æ„Ÿå’Œè¯­æ°”ä¿¡æ¯ä»¥ä¿æŒç¿»è¯‘é£æ ¼
            if 'tone' in content_analysis:
                context += f"è¯­æ°”: {content_analysis['tone']}\n"
                
            # æ·»åŠ å‰§æƒ…è®¾å®šå’Œåœºæ™¯æè¿°ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'setting' in content_analysis:
                context += f"åœºæ™¯è®¾å®š: {content_analysis['setting']}\n"
        
        # ç³»ç»Ÿæç¤ºï¼Œå¼ºè°ƒç¿»è¯‘å‡†ç¡®æ€§å’Œè‡ªç„¶æµç•…
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šç”µå½±å­—å¹•ç¿»è¯‘ä¸“å®¶ï¼Œç²¾é€š{target_language}å’Œè‹±è¯­ã€‚
è¯·å°†è‹±æ–‡å­—å¹•ç¿»è¯‘æˆ{target_language}ï¼Œéµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. å‡†ç¡®æ€§ï¼šä¿æŒåŸæ„å‡†ç¡®ä¼ è¾¾ï¼ŒåŒ…æ‹¬ä¸“ä¸šæœ¯è¯­ã€ä¹ è¯­ã€æ–‡åŒ–ç‰¹å®šè¡¨è¾¾å’ŒåŒå…³è¯­
2. è‡ªç„¶æµç•…ï¼šç¡®ä¿è¯‘æ–‡ç¬¦åˆ{target_language}çš„è¯­è¨€ä¹ æƒ¯å’Œè¡¨è¾¾æ–¹å¼
3. é£æ ¼åŒ¹é…ï¼šä¿æŒåŸä½œçš„é£æ ¼ã€è¯­æ°”å’Œè¯­åŸŸç‰¹å¾
4. æƒ…æ„Ÿè¡¨è¾¾ï¼šå‡†ç¡®ä¼ è¾¾åŸæ–‡çš„æƒ…æ„Ÿå’Œè¯­æ°”ï¼ŒåŒ…æ‹¬æ„¤æ€’ã€å¹½é»˜ã€è®½åˆºç­‰
5. æ–‡åŒ–é€‚åº”ï¼šé€‚å½“è°ƒæ•´æ–‡åŒ–ç‰¹å®šå†…å®¹ï¼Œä½¿ç›®æ ‡è¯­è¨€è§‚ä¼—èƒ½ç†è§£
6. ä¿ç•™ç²—å£å’Œä¿šè¯­ï¼šå‡†ç¡®ç¿»è¯‘ç²—å£ã€è„è¯å’Œä¿šè¯­ï¼Œä¿æŒåŸä½œçš„è¯­æ°”å’Œå¼ºåº¦
7. ç®€æ´æ€§ï¼šå­—å¹•åº”ç®€æ´æ˜äº†ï¼Œä¾¿äºè§‚ä¼—å¿«é€Ÿé˜…è¯»

æ³¨æ„ï¼Œç”µå½±å­—å¹•ç¿»è¯‘ä¸æ™®é€šæ–‡æœ¬ç¿»è¯‘ä¸åŒï¼Œè¦è€ƒè™‘è§‚çœ‹ä½“éªŒå’Œè§†å¬ä¸€è‡´æ€§ã€‚
"""

        # å¤„ç†å­—å¹•æ–‡æœ¬ï¼Œå»é™¤åºå·å’Œæ—¶é—´æˆ³
        clean_text = re.sub(r'^\d+\s+\d{2}:\d{2}:\d{2},\d{3}\s-->\s\d{2}:\d{2}:\d{2},\d{3}\s+', '', subtitle_text, flags=re.MULTILINE)
        
        # å¢åŠ ç›¸ä¼¼æ®µè½ä¸Šä¸‹æ–‡ï¼Œä¼˜å…ˆè€ƒè™‘ç´§é‚»çš„å¯¹è¯
        similar_context = ""
        if similar_chunks and len(similar_chunks) > 0:
            # å¢åŠ ç›¸ä¼¼å—æ•°é‡ï¼Œæœ€å¤šä½¿ç”¨8ä¸ªï¼ˆåŸæ¥æ˜¯3ä¸ªï¼‰
            max_chunks = min(8, len(similar_chunks))
            similar_text = "\n".join([chunk for chunk in similar_chunks[:max_chunks]])
            similar_context = f"ç›¸å…³å¯¹è¯ä¸Šä¸‹æ–‡:\n{similar_text}\n"
        
        # å¢åŠ ç›¸å…³æœ¯è¯­ä¸Šä¸‹æ–‡
        terms_context = ""
        if relevant_terms and len(relevant_terms) > 0:
            # å¢åŠ æœ€å¤§ç›¸å…³æœ¯è¯­æ•°é‡
            max_terms = min(10, len(relevant_terms))
            terms_text = ", ".join([term for term in relevant_terms[:max_terms]])
            terms_context = f"ç›¸å…³æœ¯è¯­: {terms_text}\n"
            
        # æ„å»ºæœ€ç»ˆç”¨æˆ·æç¤º
        user_prompt = f"""è¯·ç¿»è¯‘ä»¥ä¸‹ç”µå½±å­—å¹•åˆ°{target_language}ï¼Œä¿æŒè¯­æ°”ã€å¹½é»˜æ„Ÿå’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚

### å†…å®¹èƒŒæ™¯ ###
{context}

{similar_context}
{terms_context}

### è¦ç¿»è¯‘çš„å­—å¹• ###
{clean_text}

åªéœ€è¿”å›è¯‘æ–‡ï¼Œæ— éœ€è§£é‡Šã€‚ä¿ç•™åŸæ–‡çš„æ®µè½æ ¼å¼å’Œæ¢è¡Œã€‚ä¿æŒä¸“ä¸šç”µå½±å­—å¹•é£æ ¼ï¼Œç¬¦åˆç›®æ ‡è¯­è¨€çš„è¡¨è¾¾ä¹ æƒ¯ã€‚
å¯¹äºç²—å£ã€ä¿šè¯­ç­‰æƒ…æ„Ÿå¼ºçƒˆçš„è¡¨è¾¾ï¼Œè¯·ä¿æŒåŸæœ‰çš„è¯­æ°”å¼ºåº¦å’Œè¡¨è¾¾æ•ˆæœã€‚"""

        # è°ƒç”¨APIè¿›è¡Œç¿»è¯‘ - å¢åŠ å¤šæ¬¡é‡è¯•æœºåˆ¶
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # é‡è¯•æ¬¡æ•°
        max_retries = 5
        retry_count = 0
        retry_delay = 2  # åˆå§‹å»¶è¿Ÿ2ç§’
        
        # ä¸»è¦ç¿»è¯‘æ–¹æ³• - å¤šæ¬¡é‡è¯•
        while retry_count < max_retries:
            try:
                # ä¼˜åŒ–APIè°ƒç”¨å‚æ•°ï¼Œæé«˜ç¿»è¯‘è´¨é‡
                response = self.client.chat.completions.create(
                    model=self.translation_model,
                    messages=messages,
                    temperature=0.1,  # é™ä½æ¸©åº¦ï¼Œæé«˜ä¸€è‡´æ€§
                    timeout=90 + retry_count * 30,  # éšç€é‡è¯•æ¬¡æ•°å¢åŠ è¶…æ—¶æ—¶é—´
                    max_tokens=4000  # å¢åŠ æœ€å¤§ä»¤ç‰Œæ•°ï¼Œç¡®ä¿å®Œæ•´è¾“å‡º
                )
                
                # å®‰å…¨æå–ç¿»è¯‘ç»“æœ - å¢å¼ºå¯¹Noneçš„æ£€æŸ¥
                translation = ""
                if (response and hasattr(response, 'choices') and 
                    len(response.choices) > 0 and 
                    hasattr(response.choices[0], 'message') and 
                    hasattr(response.choices[0].message, 'content') and 
                    response.choices[0].message.content is not None):
                    
                    translation = response.choices[0].message.content.strip()
                
                # æ£€æŸ¥ç¿»è¯‘ç»“æœæ˜¯å¦ä¸ºç©º
                if translation:
                    if retry_count > 0:
                        print(f"âœ… ç¬¬{retry_count+1}æ¬¡å°è¯•æˆåŠŸç¿»è¯‘")
                    return translation
                else:
                    print(f"âš ï¸ è­¦å‘Š: ç¬¬{retry_count+1}æ¬¡å°è¯•ç¿»è¯‘ç»“æœä¸ºç©º")
                    retry_count += 1
                    if retry_count < max_retries:
                        print(f"ğŸ”„ ç­‰å¾…{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        retry_delay = min(retry_delay * 1.5, 10)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§å»¶è¿Ÿ10ç§’
                    continue
                    
            except Exception as e:
                print(f"âš ï¸ ç¬¬{retry_count+1}æ¬¡ç¿»è¯‘å¤±è´¥: {str(e)}")
                retry_count += 1
                if retry_count < max_retries:
                    print(f"ğŸ”„ ç­‰å¾…{retry_delay}ç§’åé‡è¯•...")
                    time.sleep(retry_delay)
                    retry_delay = min(retry_delay * 1.5, 10)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§å»¶è¿Ÿ10ç§’
                else:
                    print("âŒ ä¸»è¦ç¿»è¯‘æ–¹æ³•å¤šæ¬¡å°è¯•å¤±è´¥ï¼Œåˆ‡æ¢è‡³å¤‡ç”¨æ–¹æ³•")
                    break
        
        # å¦‚æœä¸»è¦ç¿»è¯‘æ–¹æ³•å¤šæ¬¡å°è¯•éƒ½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨ç¿»è¯‘æ–¹æ³•
        print("â„¹ï¸ ä½¿ç”¨å¤‡ç”¨ç¿»è¯‘æ–¹æ³•...")
        
        # å¤‡ç”¨ç¿»è¯‘æ–¹æ³• - ä½¿ç”¨æ›´ç®€å•çš„æç¤º
        try:
            # ä½¿ç”¨æ›´ç®€å•çš„æç¤ºè¿›è¡Œç¿»è¯‘å°è¯•
            simple_messages = [
                {"role": "system", "content": f"å°†ä»¥ä¸‹è‹±æ–‡å­—å¹•ç¿»è¯‘æˆ{target_language}ï¼Œä¿æŒåŸæ„å’Œé£æ ¼ã€‚"},
                {"role": "user", "content": clean_text}
            ]
            
            fallback_response = self.client.chat.completions.create(
                model=self.translation_model,
                messages=simple_messages,
                temperature=0.2,
                timeout=60
            )
            
            # å®‰å…¨æå–å¤‡ç”¨ç¿»è¯‘ç»“æœ
            simple_translation = ""
            if (fallback_response and hasattr(fallback_response, 'choices') and 
                len(fallback_response.choices) > 0 and 
                hasattr(fallback_response.choices[0], 'message') and 
                hasattr(fallback_response.choices[0].message, 'content') and 
                fallback_response.choices[0].message.content is not None):
                
                simple_translation = fallback_response.choices[0].message.content.strip()
            
            if simple_translation:
                print("âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æˆåŠŸç¿»è¯‘")
                return simple_translation
            else:
                print("âš ï¸ å¤‡ç”¨ç¿»è¯‘ç»“æœä¸ºç©º")
            
        except Exception as fallback_error:
            print(f"âš ï¸ å¤‡ç”¨ç¿»è¯‘ä¹Ÿå¤±è´¥: {str(fallback_error)}")
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›åŸæ–‡
        print("âš ï¸ æ‰€æœ‰ç¿»è¯‘æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›åŸæ–‡")
        return clean_text
    
    def translate_subtitle_chunk(self, 
                               text: str, 
                               target_language: str, 
                               content_analysis: Dict = None, 
                               similar_chunks: List[str] = None,
                               context_references: List[Dict] = None) -> str:
        """
        ç¿»è¯‘å­—å¹•æ–‡æœ¬å—
        
        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_language: ç›®æ ‡è¯­è¨€ä»£ç 
            content_analysis: å†…å®¹åˆ†æç»“æœ
            similar_chunks: ç›¸ä¼¼æ–‡æœ¬å—
            context_references: ä¸Šä¸‹æ–‡å‚è€ƒï¼Œç”¨äºæé«˜ç¿»è¯‘ä¸€è‡´æ€§
            
        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        # æ„å»ºç¿»è¯‘æç¤º
        system_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è§†å­—å¹•ç¿»è¯‘ä¸“å®¶ï¼Œç²¾é€šå„ç§è¯­è¨€ä¹‹é—´çš„å­—å¹•ç¿»è¯‘ã€‚
ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸‹é¢çš„å­—å¹•æ–‡æœ¬ç¿»è¯‘æˆ{target_language}ã€‚

ç¿»è¯‘å‡†åˆ™ï¼š
1. ä¿æŒåŸæ–‡çš„é£æ ¼ã€è¯­æ°”å’Œè¡¨è¾¾æ–¹å¼
2. é€‚å½“æœ¬åœ°åŒ–è¡¨è¾¾ï¼Œä½¿è¯‘æ–‡æµç•…è‡ªç„¶
3. ä¿ç•™ä¸“ä¸šæœ¯è¯­å’Œäººåçš„å‡†ç¡®æ€§
4. ä¿æŒå­—å¹•çš„ç®€æ´ï¼Œé€‚åˆè§‚ä¼—å¿«é€Ÿé˜…è¯»
5. æ³¨æ„è¯­å¢ƒï¼Œç¡®ä¿ç¿»è¯‘ç¬¦åˆå½±ç‰‡æ•´ä½“é£æ ¼
6. ä¿ç•™åŸæ–‡ä¸­çš„æ ‡ç‚¹ç¬¦å·æ ¼å¼
7. ä¿æŒä¸“ä¸šæœ¯è¯­çš„ä¸€è‡´æ€§
8. å¦‚æœå‡ºç°å¤šäººå¯¹è¯ï¼Œä¿æŒå¯¹è¯ç»“æ„å¹¶é€‚å½“æ ‡è®°

è¾“å‡ºæ ‡å‡†ï¼š
- åªè¿”å›ç¿»è¯‘åçš„æ–‡æœ¬ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–æ³¨é‡Š
- ä¸éœ€è¦è§£é‡Šä½ çš„ç¿»è¯‘é€‰æ‹©
- æ ¼å¼åº”ä¸åŸæ–‡æ ¼å¼ä¸€è‡´"""

        # æ·»åŠ å†…å®¹åˆ†æä¿¡æ¯å¢å¼ºç¿»è¯‘ä¸Šä¸‹æ–‡
        if content_analysis:
            system_prompt += f"""

å½±ç‰‡èƒŒæ™¯ä¿¡æ¯ï¼š
- å½±ç‰‡ç±»å‹ï¼š{content_analysis.get('film_type', 'æœªçŸ¥')}
- å‰§æƒ…æ‘˜è¦ï¼š{content_analysis.get('plot_summary', 'æ— ')}"""

        # å¦‚æœæœ‰å…³é”®æœ¯è¯­ï¼Œæ·»åŠ æœ¯è¯­è¡¨
        if "key_terms" in content_analysis and content_analysis["key_terms"]:
            system_prompt += "\n\nä¸“ä¸šæœ¯è¯­è¡¨ï¼ˆè¯·åœ¨ç¿»è¯‘ä¸­ä¿æŒä¸€è‡´æ€§ï¼‰ï¼š"
            for term in content_analysis["key_terms"]:
                system_prompt += f"\n- {term}"

        # æ·»åŠ ç›¸ä¼¼åœºæ™¯çš„ä¸Šä¸‹æ–‡å‚è€ƒï¼Œå¢å¼ºç¿»è¯‘ä¸€è‡´æ€§
        if context_references and len(context_references) > 0:
            system_prompt += "\n\nç›¸ä¼¼åœºæ™¯å‚è€ƒï¼ˆç”¨äºä¿æŒç¿»è¯‘é£æ ¼å’Œæœ¯è¯­ä¸€è‡´æ€§ï¼‰ï¼š"
            for i, ref in enumerate(context_references[:3]):  # æœ€å¤šä½¿ç”¨3ä¸ªå‚è€ƒ
                system_prompt += f"\nå‚è€ƒ{i+1}ï¼š\n```\n{ref['text']}\n```"
            system_prompt += "\n\nè¯·ç‰¹åˆ«æ³¨æ„ä¸ä¸Šè¿°ç›¸ä¼¼åœºæ™¯ä¿æŒé£æ ¼å’Œæœ¯è¯­çš„ä¸€è‡´æ€§ã€‚"
        # æ·»åŠ ç›¸ä¼¼å—ä¿¡æ¯
        elif similar_chunks and len(similar_chunks) > 0:
            system_prompt += "\n\nç›¸ä¼¼å†…å®¹å‚è€ƒï¼ˆç”¨äºä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰ï¼š"
            for i, chunk in enumerate(similar_chunks[:3]):  # æœ€å¤šä½¿ç”¨3ä¸ªç›¸ä¼¼å—
                system_prompt += f"\nå†…å®¹{i+1}ï¼š\n```\n{chunk}\n```"
                
        max_retries = 3
        retry_delay = 2
        
        for attempt in range(max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.translation_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": f"è¯·å°†ä»¥ä¸‹å­—å¹•ç¿»è¯‘æˆ{target_language}ï¼š\n\n{text}"}
                    ],
                    temperature=0.3,  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§
                    timeout=120,  # 2åˆ†é’Ÿè¶…æ—¶
                    max_tokens=4000
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"ç¿»è¯‘å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({attempt+1}/{max_retries}): {e}")
                    time.sleep(retry_delay)
                else:
                    print(f"ç¿»è¯‘å¤±è´¥: {e}")
                    return text  # å¤±è´¥æ—¶è¿”å›åŸæ–‡
    
    def extract_terms_from_analysis(self, content_analysis: Dict[str, Any]) -> List[str]:
        """
        ä»åˆ†æç»“æœä¸­æå–ä¸“ä¸šæœ¯è¯­
        
        Args:
            content_analysis: å†…å®¹åˆ†æç»“æœ
            
        Returns:
            æœ¯è¯­åˆ—è¡¨
        """
        if not content_analysis or "terminology" not in content_analysis:
            return []
        
        return list(content_analysis["terminology"].keys())
    
    def summarize_text(self, text: str, max_length: int = 50) -> str:
        """
        ä½¿ç”¨AIæ¨¡å‹å‹ç¼©æˆ–æ€»ç»“æ–‡æœ¬å†…å®¹
        
        Args:
            text: éœ€è¦å‹ç¼©çš„æ–‡æœ¬
            max_length: å‹ç¼©åçš„æœ€å¤§é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
            
        Returns:
            å‹ç¼©åçš„æ–‡æœ¬
        """
        if not text or len(text) <= max_length:
            return text
            
        try:
            system_prompt = f"ä½ æ˜¯ä¸€ä¸ªå­—å¹•ä¼˜åŒ–ä¸“å®¶ï¼Œéœ€è¦å°†ä»¥ä¸‹æ–‡æœ¬å‹ç¼©åˆ°{max_length}ä¸ªå­—ç¬¦ä»¥å†…ï¼ŒåŒæ—¶ä¿æŒåŸæ„ã€‚ä¿æŒåŸæ–‡çš„é£æ ¼å’Œè¯­æ°”ã€‚"
            
            response = self.client.chat.completions.create(
                model=self.translation_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"åŸæ–‡ï¼š{text}\n\nå‹ç¼©åï¼ˆä¸è¶…è¿‡{max_length}å­—ç¬¦ï¼‰ï¼š"}
                ],
                temperature=0.1,
                max_tokens=100
            )
            
            summary = response.choices[0].message.content.strip()
            
            # ç¡®ä¿ç»“æœä¸è¶…è¿‡æœ€å¤§é•¿åº¦
            if len(summary) > max_length:
                summary = summary[:max_length]
                
            return summary
            
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬å‹ç¼©å¤±è´¥: {e}")
            # å¦‚æœAIå‹ç¼©å¤±è´¥ï¼Œåˆ™è¿›è¡Œç®€å•çš„æˆªæ–­
            return text[:max_length]
            
    def translate_text(self, text: str, target_lang: str = "zh-CN") -> str:
        """
        ç®€å•ç¿»è¯‘æ–‡æœ¬ï¼Œä¸åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            text: è¦ç¿»è¯‘çš„æ–‡æœ¬
            target_lang: ç›®æ ‡è¯­è¨€
            
        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        try:
            system_prompt = f"ä½ æ˜¯ä¸€åä¸“ä¸šçš„å­—å¹•ç¿»è¯‘ä¸“å®¶ï¼Œè¯·å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆ{target_lang}ã€‚ä¿æŒåŸæ„ï¼ŒåŒæ—¶ä½¿ç¿»è¯‘è‡ªç„¶æµç•…ã€‚"
            
            response = self.client.chat.completions.create(
                model=self.translation_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"åŸæ–‡ï¼š{text}\n\nè¯‘æ–‡ï¼š"}
                ],
                temperature=0.1,
                max_tokens=200
            )
            
            translation = response.choices[0].message.content.strip()
            return translation
            
        except Exception as e:
            print(f"âš ï¸ æ–‡æœ¬ç¿»è¯‘å¤±è´¥: {e}")
            return "" 